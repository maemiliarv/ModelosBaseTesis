{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer (ViT) Training for Mammography Classification\n",
    "Dataset: DDSM + INbreast (Benignas vs Malignas)\n",
    "Model: ViT-Small from scratch\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP Y CONFIGURACIÓN\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CONFIGURACIÓN DE PATHS Y PARÁMETROS\n",
    "# ============================================================================\n",
    "\n",
    "# Paths base\n",
    "BASE_DIR = Path(\"/home/merivadeneira\")\n",
    "MASAS_DIR = BASE_DIR / \"Masas\"\n",
    "OUTPUT_DIR = BASE_DIR / \"Outputs\" / \"ViT\"\n",
    "METRICS_DIR = BASE_DIR / \"Metrics\" / \"ViT\"\n",
    "\n",
    "# Crear directorios\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuración del modelo\n",
    "CONFIG = {\n",
    "    # Datos\n",
    "    'input_channels': 1,\n",
    "    'input_size': 512,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # ViT Architecture (ViT-Small)\n",
    "    'patch_size': 16,\n",
    "    'embed_dim': 384,\n",
    "    'depth': 12,\n",
    "    'num_heads': 6,\n",
    "    'mlp_ratio': 4,\n",
    "    'dropout': 0.1,\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 100,\n",
    "    'k_folds': 5,\n",
    "    'early_stopping_patience': 25,\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_scheduler': 'ReduceLROnPlateau',\n",
    "    'scheduler_patience': 5,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'gradient_clip': 1.0,\n",
    "\n",
    "    # Augmentation\n",
    "    'rotation_degrees': 15,\n",
    "    'translate': 0.1,\n",
    "    'gaussian_blur_kernel': 5,\n",
    "    'gaussian_blur_sigma': (0.1, 0.5),\n",
    "\n",
    "    # Normalization\n",
    "    'mean': [0.5],\n",
    "    'std': [0.5],\n",
    "\n",
    "    # Model name\n",
    "    'model_name': 'ViT_0_Base'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACIÓN DEL ENTRENAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:30s}: {value}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. UTILIDADES PARA PREVENIR DATA LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "def extract_patient_id(filename, database):\n",
    "    \"\"\"\n",
    "    Extrae el patient ID del nombre del archivo para evitar data leakage\n",
    "\n",
    "    Args:\n",
    "        filename: Nombre del archivo\n",
    "        database: 'DDSM' o 'INbreast'\n",
    "\n",
    "    Returns:\n",
    "        patient_id: ID único del paciente\n",
    "    \"\"\"\n",
    "    if database == 'DDSM':\n",
    "        # Formato: P_00041_LEFT_CC_1.png -> P_00041\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"DDSM_{parts[1]}\"\n",
    "\n",
    "    elif database == 'INbreast':\n",
    "        # Formato: 20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png -> 20586908\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 1:\n",
    "            return f\"INbreast_{parts[0]}\"\n",
    "\n",
    "    return filename  # Fallback\n",
    "\n",
    "def load_image_paths_with_patient_ids():\n",
    "    \"\"\"\n",
    "    Carga todas las rutas de imágenes con sus patient IDs y labels\n",
    "\n",
    "    Returns:\n",
    "        data_df: DataFrame con columnas [image_path, patient_id, label, database]\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    # Procesar DDSM\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        ddsm_path = MASAS_DIR / \"DDSM\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if ddsm_path.exists():\n",
    "            for img_file in ddsm_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'DDSM')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'DDSM'\n",
    "                })\n",
    "\n",
    "    # Procesar INbreast\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        inbreast_path = MASAS_DIR / \"INbreast\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if inbreast_path.exists():\n",
    "            for img_file in inbreast_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'INbreast')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'INbreast'\n",
    "                })\n",
    "\n",
    "    data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    print(f\"\\nTotal de imágenes cargadas: {len(data_df)}\")\n",
    "    print(f\"  - DDSM: {len(data_df[data_df['database']=='DDSM'])}\")\n",
    "    print(f\"  - INbreast: {len(data_df[data_df['database']=='INbreast'])}\")\n",
    "    print(f\"\\nDistribución de clases:\")\n",
    "    print(f\"  - Benignas (0): {len(data_df[data_df['label']==0])}\")\n",
    "    print(f\"  - Malignas (1): {len(data_df[data_df['label']==1])}\")\n",
    "    print(f\"\\nTotal de pacientes únicos: {data_df['patient_id'].nunique()}\")\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def create_patient_level_splits(data_df, k_folds=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea splits de K-Fold a nivel de paciente (no de imagen)\n",
    "\n",
    "    Args:\n",
    "        data_df: DataFrame con información de las imágenes\n",
    "        k_folds: Número de folds\n",
    "        random_state: Semilla aleatoria\n",
    "\n",
    "    Returns:\n",
    "        fold_splits: Lista de tuplas (train_indices, val_indices)\n",
    "    \"\"\"\n",
    "    # Agrupar por paciente y obtener su label mayoritaria\n",
    "    patient_labels = data_df.groupby('patient_id')['label'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "\n",
    "    patient_labels.columns = ['patient_id', 'label']\n",
    "\n",
    "    # Crear K-Fold estratificado a nivel de paciente\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_splits = []\n",
    "    for fold_idx, (train_patient_idx, val_patient_idx) in enumerate(\n",
    "        skf.split(patient_labels['patient_id'], patient_labels['label'])\n",
    "    ):\n",
    "        # Obtener IDs de pacientes para train y val\n",
    "        train_patients = patient_labels.iloc[train_patient_idx]['patient_id'].values\n",
    "        val_patients = patient_labels.iloc[val_patient_idx]['patient_id'].values\n",
    "\n",
    "        # Obtener índices de imágenes correspondientes\n",
    "        train_indices = data_df[data_df['patient_id'].isin(train_patients)].index.tolist()\n",
    "        val_indices = data_df[data_df['patient_id'].isin(val_patients)].index.tolist()\n",
    "\n",
    "        fold_splits.append((train_indices, val_indices))\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(f\"  Train: {len(train_indices)} images from {len(train_patients)} patients\")\n",
    "        print(f\"  Val:   {len(val_indices)} images from {len(val_patients)} patients\")\n",
    "\n",
    "    return fold_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. DATASET Y TRANSFORMACIONES\n",
    "# ============================================================================\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para mamografías\"\"\"\n",
    "\n",
    "    def __init__(self, data_df, indices, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_df: DataFrame con información de las imágenes\n",
    "            indices: Lista de índices a usar\n",
    "            transform: Transformaciones a aplicar\n",
    "        \"\"\"\n",
    "        self.data = data_df.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('L')  # Grayscale\n",
    "\n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Label\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformaciones\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "\n",
    "    # Rotación\n",
    "    transforms.RandomRotation(degrees=CONFIG['rotation_degrees']),\n",
    "\n",
    "    # Traslación (sin scale ni shear)\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(CONFIG['translate'], CONFIG['translate']),\n",
    "        scale=None,\n",
    "        shear=None\n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Gaussian Blur (aplicado después de ToTensor)\n",
    "    transforms.GaussianBlur(\n",
    "        kernel_size=CONFIG['gaussian_blur_kernel'],\n",
    "        sigma=CONFIG['gaussian_blur_sigma']\n",
    "    ),\n",
    "\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. ARQUITECTURA VISION TRANSFORMER (ViT)\n",
    "# ============================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convierte imagen en secuencia de patch embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=1, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Convolución para crear patches\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches_sqrt, n_patches_sqrt)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention mechanism\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=384, num_heads=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Combine heads\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP block for Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=384, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=384, num_heads=6, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer (ViT) for image classification\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=512,\n",
    "        patch_size=16,\n",
    "        in_channels=1,\n",
    "        num_classes=2,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "\n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "\n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # (B, n_patches+1, embed_dim)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Take class token\n",
    "        logits = self.head(cls_token_final)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. LEARNING RATE FINDER\n",
    "# ============================================================================\n",
    "\n",
    "class LRFinder:\n",
    "    \"\"\"Learning Rate Range Test\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "        # Save initial state\n",
    "        self.model_state = model.state_dict()\n",
    "        self.optimizer_state = optimizer.state_dict()\n",
    "\n",
    "    def range_test(self, train_loader, start_lr=1e-7, end_lr=1, num_iter=100):\n",
    "        \"\"\"\n",
    "        Perform LR range test\n",
    "\n",
    "        Args:\n",
    "            train_loader: Training data loader\n",
    "            start_lr: Starting learning rate\n",
    "            end_lr: Ending learning rate\n",
    "            num_iter: Number of iterations\n",
    "\n",
    "        Returns:\n",
    "            lrs: List of learning rates tested\n",
    "            losses: List of corresponding losses\n",
    "        \"\"\"\n",
    "        # Reset to initial state\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "\n",
    "        # Set model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Calculate multiplication factor\n",
    "        gamma = (end_lr / start_lr) ** (1 / num_iter)\n",
    "\n",
    "        # Initialize\n",
    "        lr = start_lr\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        iterator = iter(train_loader)\n",
    "\n",
    "        for iteration in tqdm(range(num_iter), desc=\"LR Finder\"):\n",
    "            try:\n",
    "                inputs, labels = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(train_loader)\n",
    "                inputs, labels = next(iterator)\n",
    "\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Check if loss is exploding\n",
    "            if loss.item() > best_loss * 4:\n",
    "                break\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Record\n",
    "            lrs.append(lr)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Update learning rate\n",
    "            lr *= gamma\n",
    "            self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        # Restore initial state\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "\n",
    "        return lrs, losses\n",
    "\n",
    "    def plot(self, lrs, losses, skip_start=10, skip_end=5):\n",
    "        \"\"\"Plot LR vs Loss\"\"\"\n",
    "        if skip_start >= len(lrs):\n",
    "            skip_start = 0\n",
    "\n",
    "        if skip_end >= len(lrs):\n",
    "            skip_end = 0\n",
    "\n",
    "        lrs = lrs[skip_start:-skip_end] if skip_end > 0 else lrs[skip_start:]\n",
    "        losses = losses[skip_start:-skip_end] if skip_end > 0 else losses[skip_start:]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Find suggested LR (steepest descent)\n",
    "        gradients = np.gradient(losses)\n",
    "        suggested_lr_idx = np.argmin(gradients)\n",
    "        suggested_lr = lrs[suggested_lr_idx]\n",
    "\n",
    "        plt.axvline(suggested_lr, color='red', linestyle='--',\n",
    "                   label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "        plt.legend()\n",
    "\n",
    "        save_path = METRICS_DIR / f\"{CONFIG['model_name']}_lr_finder.png\"\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"\\nSuggested Learning Rate: {suggested_lr:.2e}\")\n",
    "        print(f\"LR Finder plot saved to: {save_path}\")\n",
    "\n",
    "        return suggested_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. ENTRENAMIENTO Y EVALUACIÓN\n",
    "# ============================================================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # Confusion matrix for specificity\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss stops improving\"\"\"\n",
    "\n",
    "    def __init__(self, patience=25, min_delta=0, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = lambda x, y: x < y - min_delta\n",
    "        else:\n",
    "            self.monitor_op = lambda x, y: x > y + min_delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self.monitor_op(score, self.best_score):\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def train_fold(\n",
    "    fold_idx,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    early_stopping_patience\n",
    "):\n",
    "    \"\"\"Train one fold\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_idx + 1}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience, mode='min')\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_specificity': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_specificity'].append(val_metrics['specificity'])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f} | Val Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Val Recall: {val_metrics['recall']:.4f} | Val Specificity: {val_metrics['specificity']:.4f}\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"✓ Best model updated (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping(val_metrics['loss']):\n",
    "            print(f\"\\n✓ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final validation with best model\n",
    "    final_metrics = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    return model, history, final_metrics\n",
    "\n",
    "\n",
    "def plot_training_history(history, fold_idx, save_path):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'Training History - Fold {fold_idx + 1}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # F1 Score\n",
    "    axes[0, 2].plot(epochs, history['val_f1'], 'g-', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('F1 Score')\n",
    "    axes[0, 2].set_title('Validation F1 Score')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Precision\n",
    "    axes[1, 0].plot(epochs, history['val_precision'], 'c-', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Validation Precision')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Recall\n",
    "    axes[1, 1].plot(epochs, history['val_recall'], 'm-', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].set_title('Validation Recall')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Specificity\n",
    "    axes[1, 2].plot(epochs, history['val_specificity'], 'y-', linewidth=2)\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Specificity')\n",
    "    axes[1, 2].set_title('Validation Specificity')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, fold_idx, save_path):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "\n",
    "    plt.title(f'Confusion Matrix - Fold {fold_idx + 1}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Add percentage annotations\n",
    "    total = cm.sum()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = cm[i, j] / total * 100\n",
    "            plt.text(\n",
    "                j + 0.5, i + 0.7,\n",
    "                f'({percentage:.1f}%)',\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=10,\n",
    "                color='red' if i != j else 'green'\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. FUNCIÓN PRINCIPAL DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INICIANDO ENTRENAMIENTO DE VISION TRANSFORMER\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    print(\"\\n[1/7] Cargando datos...\")\n",
    "    data_df = load_image_paths_with_patient_ids()\n",
    "\n",
    "    # 2. Crear splits por paciente\n",
    "    print(\"\\n[2/7] Creando splits K-Fold a nivel de paciente...\")\n",
    "    fold_splits = create_patient_level_splits(data_df, k_folds=CONFIG['k_folds'])\n",
    "\n",
    "    # 3. Ejecutar LR Finder en el primer fold\n",
    "    print(\"\\n[3/7] Ejecutando Learning Rate Finder...\")\n",
    "\n",
    "    train_indices, val_indices = fold_splits[0]\n",
    "    train_dataset = MammographyDataset(data_df, train_indices, train_transform)\n",
    "    temp_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Crear modelo temporal para LR Finder\n",
    "    temp_model = VisionTransformer(\n",
    "        img_size=CONFIG['input_size'],\n",
    "        patch_size=CONFIG['patch_size'],\n",
    "        in_channels=CONFIG['input_channels'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        embed_dim=CONFIG['embed_dim'],\n",
    "        depth=CONFIG['depth'],\n",
    "        num_heads=CONFIG['num_heads'],\n",
    "        mlp_ratio=CONFIG['mlp_ratio'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    temp_optimizer = AdamW(\n",
    "        temp_model.parameters(),\n",
    "        lr=1e-7,\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    temp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lr_finder = LRFinder(temp_model, temp_optimizer, temp_criterion, device)\n",
    "    lrs, losses = lr_finder.range_test(temp_loader, start_lr=1e-7, end_lr=1e-2, num_iter=100)\n",
    "    suggested_lr = lr_finder.plot(lrs, losses)\n",
    "\n",
    "    # Usar el LR sugerido\n",
    "    INITIAL_LR = min(suggested_lr / 10, 3e-4)\n",
    "    print(f\"\\n✓ Using Learning Rate: {INITIAL_LR:.2e}\")\n",
    "\n",
    "    del temp_model, temp_optimizer, temp_loader, train_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 4. Entrenamiento K-Fold\n",
    "    print(\"\\n[4/7] Iniciando entrenamiento K-Fold...\")\n",
    "\n",
    "    all_fold_metrics = []\n",
    "    all_confusion_matrices = []\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_splits):\n",
    "\n",
    "        # Crear datasets y dataloaders\n",
    "        train_dataset = MammographyDataset(data_df, train_indices, train_transform)\n",
    "        val_dataset = MammographyDataset(data_df, val_indices, val_transform)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Crear modelo\n",
    "        model = VisionTransformer(\n",
    "            img_size=CONFIG['input_size'],\n",
    "            patch_size=CONFIG['patch_size'],\n",
    "            in_channels=CONFIG['input_channels'],\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            embed_dim=CONFIG['embed_dim'],\n",
    "            depth=CONFIG['depth'],\n",
    "            num_heads=CONFIG['num_heads'],\n",
    "            mlp_ratio=CONFIG['mlp_ratio'],\n",
    "            dropout=CONFIG['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer y scheduler\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=INITIAL_LR,\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=CONFIG['scheduler_factor'],\n",
    "            patience=CONFIG['scheduler_patience'],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Entrenar fold\n",
    "        model, history, final_metrics = train_fold(\n",
    "            fold_idx=fold_idx,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=CONFIG['num_epochs'],\n",
    "            early_stopping_patience=CONFIG['early_stopping_patience']\n",
    "        )\n",
    "\n",
    "        # Guardar modelo\n",
    "        model_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}.pth\"\n",
    "        torch.save({\n",
    "            'fold': fold_idx,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': CONFIG,\n",
    "            'final_metrics': final_metrics\n",
    "        }, model_path)\n",
    "        print(f\"\\n✓ Model saved: {model_path}\")\n",
    "\n",
    "        # Guardar gráficos\n",
    "        history_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}_history.png\"\n",
    "        plot_training_history(history, fold_idx, history_plot_path)\n",
    "        print(f\"✓ History plot saved: {history_plot_path}\")\n",
    "\n",
    "        cm_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}_confusion_matrix.png\"\n",
    "        plot_confusion_matrix(final_metrics['confusion_matrix'], fold_idx, cm_plot_path)\n",
    "        print(f\"✓ Confusion matrix saved: {cm_plot_path}\")\n",
    "\n",
    "        # Guardar métricas\n",
    "        all_fold_metrics.append(final_metrics)\n",
    "        all_confusion_matrices.append(final_metrics['confusion_matrix'])\n",
    "\n",
    "        # Limpiar memoria\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # 5. Calcular métricas promedio\n",
    "    print(\"\\n[5/7] Calculando métricas promedio...\")\n",
    "\n",
    "    metrics_summary = {\n",
    "        'f1': [m['f1'] for m in all_fold_metrics],\n",
    "        'precision': [m['precision'] for m in all_fold_metrics],\n",
    "        'recall': [m['recall'] for m in all_fold_metrics],\n",
    "        'specificity': [m['specificity'] for m in all_fold_metrics],\n",
    "        'accuracy': [m['accuracy'] for m in all_fold_metrics]\n",
    "    }\n",
    "\n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['F1-Score', 'Precision', 'Recall', 'Specificity', 'Accuracy'],\n",
    "        'Mean': [\n",
    "            np.mean(metrics_summary['f1']),\n",
    "            np.mean(metrics_summary['precision']),\n",
    "            np.mean(metrics_summary['recall']),\n",
    "            np.mean(metrics_summary['specificity']),\n",
    "            np.mean(metrics_summary['accuracy'])\n",
    "        ],\n",
    "        'Std': [\n",
    "            np.std(metrics_summary['f1']),\n",
    "            np.std(metrics_summary['precision']),\n",
    "            np.std(metrics_summary['recall']),\n",
    "            np.std(metrics_summary['specificity']),\n",
    "            np.std(metrics_summary['accuracy'])\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Guardar métricas\n",
    "    metrics_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    print(f\"\\n✓ Metrics saved: {metrics_csv_path}\")\n",
    "\n",
    "    # Imprimir métricas\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MÉTRICAS PROMEDIO (10-FOLD CROSS VALIDATION)\")\n",
    "    print(\"=\"*70)\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 6. Crear matriz de confusión promedio\n",
    "    print(\"\\n[6/7] Creando matriz de confusión promedio...\")\n",
    "\n",
    "    mean_cm = np.mean(all_confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        mean_cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.title(f'Average Confusion Matrix - {CONFIG[\"model_name\"]}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Add metrics text\n",
    "    metrics_text = f\"\"\"\n",
    "    Mean Metrics:\n",
    "    F1-Score: {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\n",
    "    Precision: {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\n",
    "    Recall: {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\n",
    "    Specificity: {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(\n",
    "        2.5, 0.5, metrics_text,\n",
    "        fontsize=10,\n",
    "        verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "    mean_cm_path = METRICS_DIR / f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(mean_cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Mean confusion matrix saved: {mean_cm_path}\")\n",
    "\n",
    "    # 7. Crear resumen visual de todos los folds\n",
    "    print(\"\\n[7/7] Creando resumen visual...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'10-Fold Cross Validation Summary - {CONFIG[\"model_name\"]}',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Gráfico 1: F1-Score por fold\n",
    "    folds = range(1, CONFIG['k_folds'] + 1)\n",
    "    axes[0, 0].bar(folds, metrics_summary['f1'], color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].axhline(\n",
    "        np.mean(metrics_summary['f1']),\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        label=f\"Mean: {np.mean(metrics_summary['f1']):.4f}\"\n",
    "    )\n",
    "    axes[0, 0].set_xlabel('Fold')\n",
    "    axes[0, 0].set_ylabel('F1-Score')\n",
    "    axes[0, 0].set_title('F1-Score per Fold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 2: Todas las métricas por fold\n",
    "    x = np.arange(CONFIG['k_folds'])\n",
    "    width = 0.15\n",
    "\n",
    "    axes[0, 1].bar(x - 2*width, metrics_summary['f1'], width, label='F1', alpha=0.8)\n",
    "    axes[0, 1].bar(x - width, metrics_summary['precision'], width, label='Precision', alpha=0.8)\n",
    "    axes[0, 1].bar(x, metrics_summary['recall'], width, label='Recall', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width, metrics_summary['specificity'], width, label='Specificity', alpha=0.8)\n",
    "    axes[0, 1].bar(x + 2*width, metrics_summary['accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "\n",
    "    axes[0, 1].set_xlabel('Fold')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('All Metrics per Fold')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels([f'{i+1}' for i in range(CONFIG['k_folds'])])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 3: Box plot de métricas\n",
    "    metrics_data = [\n",
    "        metrics_summary['f1'],\n",
    "        metrics_summary['precision'],\n",
    "        metrics_summary['recall'],\n",
    "        metrics_summary['specificity'],\n",
    "        metrics_summary['accuracy']\n",
    "    ]\n",
    "\n",
    "    bp = axes[1, 0].boxplot(\n",
    "        metrics_data,\n",
    "        labels=['F1', 'Precision', 'Recall', 'Specificity', 'Accuracy'],\n",
    "        patch_artist=True\n",
    "    )\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Metrics Distribution')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 4: Tabla de resumen\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    summary_text = f\"\"\"\n",
    "    MODEL: {CONFIG['model_name']}\n",
    "    CONFIGURATION:\n",
    "    • Architecture: ViT-Small\n",
    "    • Embed Dim: {CONFIG['embed_dim']}\n",
    "    • Depth: {CONFIG['depth']}\n",
    "    • Heads: {CONFIG['num_heads']}\n",
    "    • Patch Size: {CONFIG['patch_size']}\n",
    "    • Input Size: {CONFIG['input_size']}×{CONFIG['input_size']}\n",
    "\n",
    "    TRAINING:\n",
    "    • Folds: {CONFIG['k_folds']}\n",
    "    • Max Epochs: {CONFIG['num_epochs']}\n",
    "    • Batch Size: {CONFIG['batch_size']}\n",
    "    • Initial LR: {INITIAL_LR:.2e}\n",
    "    • Weight Decay: {CONFIG['weight_decay']}\n",
    "    • Early Stop Patience: {CONFIG['early_stopping_patience']}\n",
    "\n",
    "    RESULTS (Mean ± Std):\n",
    "    • F1-Score:     {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\n",
    "    • Precision:    {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\n",
    "    • Recall:       {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\n",
    "    • Specificity:  {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\n",
    "    • Accuracy:     {np.mean(metrics_summary['accuracy']):.4f} ± {np.std(metrics_summary['accuracy']):.4f}\n",
    "    \"\"\"\n",
    "\n",
    "    axes[1, 1].text(\n",
    "        0.1, 0.5, summary_text,\n",
    "        transform=axes[1, 1].transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='center',\n",
    "        fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
    "    )\n",
    "\n",
    "    summary_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_summary.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Summary plot saved: {summary_plot_path}\")\n",
    "\n",
    "    # Guardar configuración completa\n",
    "    config_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        config_to_save = CONFIG.copy()\n",
    "        config_to_save['initial_lr'] = float(INITIAL_LR)\n",
    "        json.dump(config_to_save, f, indent=4)\n",
    "    print(f\"✓ Configuration saved: {config_path}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nModelos guardados en: {OUTPUT_DIR}\")\n",
    "    print(f\"Métricas guardadas en: {METRICS_DIR}\")\n",
    "    print(\"\\nArchivos generados:\")\n",
    "    print(f\"  • {CONFIG['k_folds']} modelos (.pth)\")\n",
    "    print(f\"  • {CONFIG['k_folds']} gráficos de historial\")\n",
    "    print(f\"  • {CONFIG['k_folds']} matrices de confusión\")\n",
    "    print(f\"  • 1 tabla de métricas (CSV)\")\n",
    "    print(f\"  • 1 matriz de confusión promedio\")\n",
    "    print(f\"  • 1 resumen visual completo\")\n",
    "    print(f\"  • 1 configuración (JSON)\")\n",
    "    print(f\"  • 1 gráfico de LR Finder\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. EJECUTAR ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
