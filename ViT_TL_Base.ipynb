{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. SETUP Y CONFIGURACIÓN\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    # Optimizaciones de memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # Configurar asignación de memoria expandible\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "    print(\"\\n✓ Optimizaciones de memoria aplicadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CONFIGURACIÓN DE PATHS Y PARÁMETROS\n",
    "# ============================================================================\n",
    "\n",
    "# Paths base\n",
    "BASE_DIR = Path(\"/home/merivadeneira\")\n",
    "MASAS_DIR = BASE_DIR / \"Masas\"\n",
    "OUTPUT_DIR = BASE_DIR / \"Outputs\" / \"ViT\"\n",
    "METRICS_DIR = BASE_DIR / \"Metrics\" / \"ViT\"\n",
    "\n",
    "# Crear directorios\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuración del modelo\n",
    "CONFIG = {\n",
    "    # Datos\n",
    "    'input_channels': 3,  # ViT pre-entrenado requiere 3 canales (RGB)\n",
    "    'input_size': 224,    # Tamaño estándar para ViT pre-entrenado\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Transfer Learning\n",
    "    'pretrained_model': 'google/vit-base-patch16-224',\n",
    "    'freeze_backbone': True,  # Congelar capas pre-entrenadas inicialmente\n",
    "    'unfreeze_after_epoch': 10,  # Descongelar después de 10 épocas\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 100,\n",
    "    'k_folds': 5,\n",
    "    'early_stopping_patience': 25,\n",
    "\n",
    "    # Optimizer (LR más bajo para transfer learning)\n",
    "    'optimizer': 'AdamW',\n",
    "    'base_lr': 1e-4,  # LR para clasificador\n",
    "    'backbone_lr': 1e-5,  # LR para backbone (cuando se descongele)\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_scheduler': 'ReduceLROnPlateau',\n",
    "    'scheduler_patience': 5,\n",
    "    'scheduler_factor': 0.5,\n",
    "\n",
    "    # Augmentation\n",
    "    'rotation_degrees': 15,\n",
    "    'translate': 0.1,\n",
    "    'gaussian_blur_kernel': 5,  # Kernel size para Gaussian Blur\n",
    "    'gaussian_blur_sigma': (0.1, 0.5),  # Rango de sigma (bajo)\n",
    "\n",
    "    # Normalization (ImageNet stats para transfer learning)\n",
    "    'mean': [0.485, 0.456, 0.406],  # ImageNet mean\n",
    "    'std': [0.229, 0.224, 0.225],   # ImageNet std\n",
    "\n",
    "    # Model name\n",
    "    'model_name': 'ViT_TL_Base'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACIÓN DEL ENTRENAMIENTO - TRANSFER LEARNING\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:30s}: {value}\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. UTILIDADES PARA PREVENIR DATA LEAKAGE (IGUAL QUE ANTES)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_patient_id(filename, database):\n",
    "    \"\"\"\n",
    "    Extrae el patient ID del nombre del archivo para evitar data leakage\n",
    "    \"\"\"\n",
    "    if database == 'DDSM':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"DDSM_{parts[1]}\"\n",
    "\n",
    "    elif database == 'INbreast':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 1:\n",
    "            return f\"INbreast_{parts[0]}\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "def load_image_paths_with_patient_ids():\n",
    "    \"\"\"\n",
    "    Carga todas las rutas de imágenes con sus patient IDs y labels\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    # Procesar DDSM\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        ddsm_path = MASAS_DIR / \"DDSM\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if ddsm_path.exists():\n",
    "            for img_file in ddsm_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'DDSM')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'DDSM'\n",
    "                })\n",
    "\n",
    "    # Procesar INbreast\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        inbreast_path = MASAS_DIR / \"INbreast\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if inbreast_path.exists():\n",
    "            for img_file in inbreast_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'INbreast')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'INbreast'\n",
    "                })\n",
    "\n",
    "    data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    print(f\"\\nTotal de imágenes cargadas: {len(data_df)}\")\n",
    "    print(f\"  - DDSM: {len(data_df[data_df['database']=='DDSM'])}\")\n",
    "    print(f\"  - INbreast: {len(data_df[data_df['database']=='INbreast'])}\")\n",
    "    print(f\"\\nDistribución de clases:\")\n",
    "    print(f\"  - Benignas (0): {len(data_df[data_df['label']==0])}\")\n",
    "    print(f\"  - Malignas (1): {len(data_df[data_df['label']==1])}\")\n",
    "    print(f\"\\nTotal de pacientes únicos: {data_df['patient_id'].nunique()}\")\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def create_patient_level_splits(data_df, k_folds=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea splits de K-Fold a nivel de paciente (no de imagen)\n",
    "    \"\"\"\n",
    "    patient_labels = data_df.groupby('patient_id')['label'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "\n",
    "    patient_labels.columns = ['patient_id', 'label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_splits = []\n",
    "    for fold_idx, (train_patient_idx, val_patient_idx) in enumerate(\n",
    "        skf.split(patient_labels['patient_id'], patient_labels['label'])\n",
    "    ):\n",
    "        train_patients = patient_labels.iloc[train_patient_idx]['patient_id'].values\n",
    "        val_patients = patient_labels.iloc[val_patient_idx]['patient_id'].values\n",
    "\n",
    "        train_indices = data_df[data_df['patient_id'].isin(train_patients)].index.tolist()\n",
    "        val_indices = data_df[data_df['patient_id'].isin(val_patients)].index.tolist()\n",
    "\n",
    "        fold_splits.append((train_indices, val_indices))\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(f\"  Train: {len(train_indices)} images from {len(train_patients)} patients\")\n",
    "        print(f\"  Val:   {len(val_indices)} images from {len(val_patients)} patients\")\n",
    "\n",
    "    return fold_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. DATASET Y TRANSFORMACIONES\n",
    "# ============================================================================\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para mamografías\"\"\"\n",
    "\n",
    "    def __init__(self, data_df, indices, transform=None):\n",
    "        self.data = data_df.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen en grayscale\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('L')\n",
    "\n",
    "        # Convertir a RGB (3 canales) para ViT pre-entrenado\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformaciones\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "\n",
    "    # Rotación\n",
    "    transforms.RandomRotation(degrees=CONFIG['rotation_degrees']),\n",
    "\n",
    "    # Traslación (sin scale ni shear)\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(CONFIG['translate'], CONFIG['translate']),\n",
    "        scale=None,\n",
    "        shear=None\n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Gaussian Blur (aplicado después de ToTensor)\n",
    "    transforms.GaussianBlur(\n",
    "        kernel_size=CONFIG['gaussian_blur_kernel'],\n",
    "        sigma=CONFIG['gaussian_blur_sigma']\n",
    "    ),\n",
    "\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. MODELO CON TRANSFER LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "class ViTForMammography(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT con Transfer Learning para clasificación de mamografías\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name='google/vit-base-patch16-224',\n",
    "        num_classes=2,\n",
    "        freeze_backbone=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Cargar modelo pre-entrenado\n",
    "        print(f\"Cargando modelo pre-entrenado: {pretrained_model_name}\")\n",
    "        self.vit = ViTModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Congelar backbone si se especifica\n",
    "        if freeze_backbone:\n",
    "            self.freeze_backbone()\n",
    "            print(\"✓ Backbone congelado\")\n",
    "\n",
    "        # Clasificador personalizado\n",
    "        hidden_size = self.vit.config.hidden_size  # 768 para ViT-Base\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Modelo cargado con {self.count_parameters():,} parámetros\")\n",
    "        print(f\"  - Backbone: {self.count_parameters(self.vit):,} parámetros\")\n",
    "        print(f\"  - Classifier: {self.count_parameters(self.classifier):,} parámetros\")\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Congela el backbone pre-entrenado\"\"\"\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Descongela el backbone para fine-tuning\"\"\"\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"✓ Backbone descongelado para fine-tuning\")\n",
    "\n",
    "    def count_parameters(self, model=None):\n",
    "        \"\"\"Cuenta parámetros entrenables\"\"\"\n",
    "        if model is None:\n",
    "            model = self\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Pasar por ViT backbone\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "\n",
    "        # Obtener CLS token (primer token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Clasificación\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. ENTRENAMIENTO Y EVALUACIÓN\n",
    "# ============================================================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    # Confusion matrix for specificity\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping\"\"\"\n",
    "\n",
    "    def __init__(self, patience=25, min_delta=0, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = lambda x, y: x < y - min_delta\n",
    "        else:\n",
    "            self.monitor_op = lambda x, y: x > y + min_delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self.monitor_op(score, self.best_score):\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def train_fold(\n",
    "    fold_idx,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    early_stopping_patience,\n",
    "    unfreeze_after_epoch\n",
    "):\n",
    "    \"\"\"Train one fold with optional backbone unfreezing\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_idx + 1}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience, mode='min')\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    backbone_unfrozen = False\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_specificity': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Descongelar backbone después de N épocas\n",
    "        if epoch == unfreeze_after_epoch and not backbone_unfrozen:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"DESCONGELANDO BACKBONE PARA FINE-TUNING\")\n",
    "            print(f\"{'='*70}\")\n",
    "            model.unfreeze_backbone()\n",
    "\n",
    "            # Ajustar optimizer con diferentes LRs\n",
    "            optimizer = AdamW([\n",
    "                {'params': model.vit.parameters(), 'lr': CONFIG['backbone_lr']},\n",
    "                {'params': model.classifier.parameters(), 'lr': CONFIG['base_lr']}\n",
    "            ], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "            backbone_unfrozen = True\n",
    "            print(f\"Backbone LR: {CONFIG['backbone_lr']:.2e}\")\n",
    "            print(f\"Classifier LR: {CONFIG['base_lr']:.2e}\\n\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_specificity'].append(val_metrics['specificity'])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f} | Val Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Val Recall: {val_metrics['recall']:.4f} | Val Specificity: {val_metrics['specificity']:.4f}\")\n",
    "\n",
    "        if backbone_unfrozen:\n",
    "            print(f\"Backbone LR: {optimizer.param_groups[0]['lr']:.2e} | Classifier LR: {optimizer.param_groups[1]['lr']:.2e}\")\n",
    "        else:\n",
    "            print(f\"Classifier LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"✓ Best model updated (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping(val_metrics['loss']):\n",
    "            print(f\"\\n✓ Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final validation with best model\n",
    "    final_metrics = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    return model, history, final_metrics\n",
    "\n",
    "\n",
    "def plot_training_history(history, fold_idx, save_path):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'Training History - Fold {fold_idx + 1}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # F1 Score\n",
    "    axes[0, 2].plot(epochs, history['val_f1'], 'g-', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('F1 Score')\n",
    "    axes[0, 2].set_title('Validation F1 Score')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Precision\n",
    "    axes[1, 0].plot(epochs, history['val_precision'], 'c-', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Validation Precision')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Recall\n",
    "    axes[1, 1].plot(epochs, history['val_recall'], 'm-', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].set_title('Validation Recall')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Specificity\n",
    "    axes[1, 2].plot(epochs, history['val_specificity'], 'y-', linewidth=2)\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Specificity')\n",
    "    axes[1, 2].set_title('Validation Specificity')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, fold_idx, save_path):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "\n",
    "    plt.title(f'Confusion Matrix - Fold {fold_idx + 1}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Add percentage annotations\n",
    "    total = cm.sum()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = cm[i, j] / total * 100\n",
    "            plt.text(\n",
    "                j + 0.5, i + 0.7,\n",
    "                f'({percentage:.1f}%)',\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=10,\n",
    "                color='red' if i != j else 'green'\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. FUNCIÓN PRINCIPAL DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INICIANDO ENTRENAMIENTO - VISION TRANSFORMER CON TRANSFER LEARNING\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    print(\"\\n[1/5] Cargando datos...\")\n",
    "    data_df = load_image_paths_with_patient_ids()\n",
    "\n",
    "    # 2. Crear splits por paciente\n",
    "    print(\"\\n[2/5] Creando splits K-Fold a nivel de paciente...\")\n",
    "    fold_splits = create_patient_level_splits(data_df, k_folds=CONFIG['k_folds'])\n",
    "\n",
    "    # 3. Entrenamiento K-Fold\n",
    "    print(\"\\n[3/5] Iniciando entrenamiento K-Fold...\")\n",
    "\n",
    "    all_fold_metrics = []\n",
    "    all_confusion_matrices = []\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_splits):\n",
    "\n",
    "        # Limpiar memoria\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Crear datasets y dataloaders\n",
    "        train_dataset = MammographyDataset(data_df, train_indices, train_transform)\n",
    "        val_dataset = MammographyDataset(data_df, val_indices, val_transform)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Crear modelo\n",
    "        model = ViTForMammography(\n",
    "            pretrained_model_name=CONFIG['pretrained_model'],\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            freeze_backbone=CONFIG['freeze_backbone']\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer (solo para el clasificador inicialmente)\n",
    "        optimizer = AdamW(\n",
    "            model.classifier.parameters(),\n",
    "            lr=CONFIG['base_lr'],\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=CONFIG['scheduler_factor'],\n",
    "            patience=CONFIG['scheduler_patience'],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Entrenar fold\n",
    "        model, history, final_metrics = train_fold(\n",
    "            fold_idx=fold_idx,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=CONFIG['num_epochs'],\n",
    "            early_stopping_patience=CONFIG['early_stopping_patience'],\n",
    "            unfreeze_after_epoch=CONFIG['unfreeze_after_epoch']\n",
    "        )\n",
    "\n",
    "        # Guardar modelo\n",
    "        model_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}.pth\"\n",
    "        torch.save({\n",
    "            'fold': fold_idx,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': CONFIG,\n",
    "            'final_metrics': final_metrics\n",
    "        }, model_path)\n",
    "        print(f\"\\n✓ Model saved: {model_path}\")\n",
    "\n",
    "        # Guardar gráficos\n",
    "        history_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}_history.png\"\n",
    "        plot_training_history(history, fold_idx, history_plot_path)\n",
    "        print(f\"✓ History plot saved: {history_plot_path}\")\n",
    "\n",
    "        cm_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}_confusion_matrix.png\"\n",
    "        plot_confusion_matrix(final_metrics['confusion_matrix'], fold_idx, cm_plot_path)\n",
    "        print(f\"✓ Confusion matrix saved: {cm_plot_path}\")\n",
    "\n",
    "        # Guardar métricas\n",
    "        all_fold_metrics.append(final_metrics)\n",
    "        all_confusion_matrices.append(final_metrics['confusion_matrix'])\n",
    "\n",
    "        # Limpiar memoria\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # 4. Calcular métricas promedio\n",
    "    print(\"\\n[4/5] Calculando métricas promedio...\")\n",
    "\n",
    "    metrics_summary = {\n",
    "        'f1': [m['f1'] for m in all_fold_metrics],\n",
    "        'precision': [m['precision'] for m in all_fold_metrics],\n",
    "        'recall': [m['recall'] for m in all_fold_metrics],\n",
    "        'specificity': [m['specificity'] for m in all_fold_metrics],\n",
    "        'accuracy': [m['accuracy'] for m in all_fold_metrics]\n",
    "    }\n",
    "\n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['F1-Score', 'Precision', 'Recall', 'Specificity', 'Accuracy'],\n",
    "        'Mean': [\n",
    "            np.mean(metrics_summary['f1']),\n",
    "            np.mean(metrics_summary['precision']),\n",
    "            np.mean(metrics_summary['recall']),\n",
    "            np.mean(metrics_summary['specificity']),\n",
    "            np.mean(metrics_summary['accuracy'])\n",
    "        ],\n",
    "        'Std': [\n",
    "            np.std(metrics_summary['f1']),\n",
    "            np.std(metrics_summary['precision']),\n",
    "            np.std(metrics_summary['recall']),\n",
    "            np.std(metrics_summary['specificity']),\n",
    "            np.std(metrics_summary['accuracy'])\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Guardar métricas\n",
    "    metrics_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    print(f\"\\n✓ Metrics saved: {metrics_csv_path}\")\n",
    "\n",
    "    # Imprimir métricas\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MÉTRICAS PROMEDIO (10-FOLD CROSS VALIDATION)\")\n",
    "    print(\"=\"*70)\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 5. Crear visualizaciones finales\n",
    "    print(\"\\n[5/5] Creando visualizaciones finales...\")\n",
    "\n",
    "    # Matriz de confusión promedio\n",
    "    mean_cm = np.mean(all_confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        mean_cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.title(f'Average Confusion Matrix - {CONFIG[\"model_name\"]}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Add metrics text\n",
    "    metrics_text = f\"\"\"\n",
    "    Mean Metrics:\n",
    "    F1-Score: {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\n",
    "    Precision: {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\n",
    "    Recall: {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\n",
    "    Specificity: {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(\n",
    "        2.5, 0.5, metrics_text,\n",
    "        fontsize=10,\n",
    "        verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "    mean_cm_path = METRICS_DIR / f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(mean_cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Mean confusion matrix saved: {mean_cm_path}\")\n",
    "\n",
    "    # Resumen visual completo\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'10-Fold Cross Validation Summary - {CONFIG[\"model_name\"]}',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Gráfico 1: F1-Score por fold\n",
    "    folds = range(1, CONFIG['k_folds'] + 1)\n",
    "    axes[0, 0].bar(folds, metrics_summary['f1'], color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].axhline(\n",
    "        np.mean(metrics_summary['f1']),\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        label=f\"Mean: {np.mean(metrics_summary['f1']):.4f}\"\n",
    "    )\n",
    "    axes[0, 0].set_xlabel('Fold')\n",
    "    axes[0, 0].set_ylabel('F1-Score')\n",
    "    axes[0, 0].set_title('F1-Score per Fold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 2: Todas las métricas por fold\n",
    "    x = np.arange(CONFIG['k_folds'])\n",
    "    width = 0.15\n",
    "\n",
    "    axes[0, 1].bar(x - 2*width, metrics_summary['f1'], width, label='F1', alpha=0.8)\n",
    "    axes[0, 1].bar(x - width, metrics_summary['precision'], width, label='Precision', alpha=0.8)\n",
    "    axes[0, 1].bar(x, metrics_summary['recall'], width, label='Recall', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width, metrics_summary['specificity'], width, label='Specificity', alpha=0.8)\n",
    "    axes[0, 1].bar(x + 2*width, metrics_summary['accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "\n",
    "    axes[0, 1].set_xlabel('Fold')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('All Metrics per Fold')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels([f'{i+1}' for i in range(CONFIG['k_folds'])])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 3: Box plot de métricas\n",
    "    metrics_data = [\n",
    "        metrics_summary['f1'],\n",
    "        metrics_summary['precision'],\n",
    "        metrics_summary['recall'],\n",
    "        metrics_summary['specificity'],\n",
    "        metrics_summary['accuracy']\n",
    "    ]\n",
    "\n",
    "    bp = axes[1, 0].boxplot(\n",
    "        metrics_data,\n",
    "        labels=['F1', 'Precision', 'Recall', 'Specificity', 'Accuracy'],\n",
    "        patch_artist=True\n",
    "    )\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Metrics Distribution')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Gráfico 4: Tabla de resumen\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    summary_text = f\"\"\"\n",
    "    MODEL: {CONFIG['model_name']}\n",
    "\n",
    "    TRANSFER LEARNING CONFIGURATION:\n",
    "    • Base Model: {CONFIG['pretrained_model']}\n",
    "    • Input Size: {CONFIG['input_size']}×{CONFIG['input_size']}\n",
    "    • Initial Backbone: Frozen\n",
    "    • Unfreeze After: Epoch {CONFIG['unfreeze_after_epoch']}\n",
    "\n",
    "    TRAINING:\n",
    "    • Folds: {CONFIG['k_folds']}\n",
    "    • Max Epochs: {CONFIG['num_epochs']}\n",
    "    • Batch Size: {CONFIG['batch_size']}\n",
    "    • Base LR: {CONFIG['base_lr']:.2e}\n",
    "    • Backbone LR: {CONFIG['backbone_lr']:.2e}\n",
    "    • Weight Decay: {CONFIG['weight_decay']}\n",
    "    • Early Stop Patience: {CONFIG['early_stopping_patience']}\n",
    "\n",
    "    RESULTS (Mean ± Std):\n",
    "    • F1-Score:     {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\n",
    "    • Precision:    {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\n",
    "    • Recall:       {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\n",
    "    • Specificity:  {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\n",
    "    • Accuracy:     {np.mean(metrics_summary['accuracy']):.4f} ± {np.std(metrics_summary['accuracy']):.4f}\n",
    "\n",
    "    ADVANTAGES OF TRANSFER LEARNING:\n",
    "    ✓ Pre-trained on ImageNet-21k\n",
    "    ✓ Faster convergence\n",
    "    ✓ Better generalization\n",
    "    ✓ Requires less data\n",
    "    \"\"\"\n",
    "\n",
    "    axes[1, 1].text(\n",
    "        0.1, 0.5, summary_text,\n",
    "        transform=axes[1, 1].transAxes,\n",
    "        fontsize=9,\n",
    "        verticalalignment='center',\n",
    "        fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
    "    )\n",
    "\n",
    "    summary_plot_path = METRICS_DIR / f\"{CONFIG['model_name']}_summary.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Summary plot saved: {summary_plot_path}\")\n",
    "\n",
    "    # Guardar configuración completa\n",
    "    config_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=4)\n",
    "    print(f\"✓ Configuration saved: {config_path}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ ENTRENAMIENTO COMPLETADO - TRANSFER LEARNING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nModelos guardados en: {OUTPUT_DIR}\")\n",
    "    print(f\"Métricas guardadas en: {METRICS_DIR}\")\n",
    "    print(\"\\nArchivos generados:\")\n",
    "    print(f\"  • {CONFIG['k_folds']} modelos (.pth)\")\n",
    "    print(f\"  • {CONFIG['k_folds']} gráficos de historial\")\n",
    "    print(f\"  • {CONFIG['k_folds']} matrices de confusión\")\n",
    "    print(f\"  • 1 tabla de métricas (CSV)\")\n",
    "    print(f\"  • 1 matriz de confusión promedio\")\n",
    "    print(f\"  • 1 resumen visual completo\")\n",
    "    print(f\"  • 1 configuración (JSON)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Comparación con modelo desde cero (si existe)\n",
    "    vit_base_metrics_path = METRICS_DIR / \"ViT_0_Base_metrics.csv\"\n",
    "    if vit_base_metrics_path.exists():\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARACIÓN: ViT desde cero vs Transfer Learning\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        vit_base_df = pd.read_csv(vit_base_metrics_path)\n",
    "\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Metric': metrics_df['Metric'],\n",
    "            'ViT_0_Base (Mean)': vit_base_df['Mean'],\n",
    "            'ViT_TL_Base (Mean)': metrics_df['Mean'],\n",
    "            'Improvement': metrics_df['Mean'] - vit_base_df['Mean']\n",
    "        })\n",
    "\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Guardar comparación\n",
    "        comparison_path = METRICS_DIR / \"comparison_ViT_vs_ViT_TL.csv\"\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        print(f\"\\n✓ Comparison saved: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. EJECUTAR ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
