{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar Hugging Face Transformers\n",
    "!pip install transformers datasets pillow -q\n",
    "\n",
    "print(\"‚úì Transformers instalado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CvtConfig, CvtModel\n",
    "\n",
    "print(\"Verificando modelos CvT en Hugging Face...\\n\")\n",
    "\n",
    "cvt_models = [\n",
    "    'microsoft/cvt-13',    # 20M params - RECOMENDADO\n",
    "    'microsoft/cvt-21',    # 32M params\n",
    "    'microsoft/cvt-w24'    # 277M params - MUY GRANDE\n",
    "]\n",
    "\n",
    "for model_name in cvt_models:\n",
    "    try:\n",
    "        config = CvtConfig.from_pretrained(model_name)\n",
    "        print(f\"‚úì {model_name}\")\n",
    "        print(f\"  Depth: {config.depth}\")\n",
    "        print(f\"  Embed dims: {config.embed_dim}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {model_name} - Error\\n\")\n",
    "\n",
    "print(\"üí° Recomendado: microsoft/cvt-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LinearLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import CvtModel, CvtConfig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports loaded successfully (using Hugging Face Transformers)\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU verification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úì GPU cache cleared\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU available, training will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration para CvT con Hugging Face (512x512)\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'base_path': '/home/merivadeneira',\n",
    "    'ddsm_benign_path': '/home/merivadeneira/Masas/DDSM/Benignas/Resized_512',\n",
    "    'ddsm_malign_path': '/home/merivadeneira/Masas/DDSM/Malignas/Resized_512',\n",
    "    'inbreast_benign_path': '/home/merivadeneira/Masas/INbreast/Benignas/Resized_512',\n",
    "    'inbreast_malign_path': '/home/merivadeneira/Masas/INbreast/Malignas/Resized_512',\n",
    "    'output_dir': '/home/merivadeneira/Outputs/CvT',\n",
    "    'metrics_dir': '/home/merivadeneira/Metrics/CvT',\n",
    "\n",
    "    # Model (HUGGING FACE)\n",
    "    'model_name': 'CvT_TL_Base_HF',\n",
    "    'pretrained_model': 'microsoft/cvt-13',\n",
    "    'pretrained': True,\n",
    "    'input_size': 512,\n",
    "    'in_channels': 3,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Fine-tuning strategy\n",
    "    'freeze_strategy': 'none',\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 100,\n",
    "    'num_folds': 5,\n",
    "    'early_stopping_patience': 25,\n",
    "    'min_delta': 1e-4,\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr_initial': None,\n",
    "    'lr_min': 1e-7,\n",
    "    'lr_max': 1e-3,\n",
    "    'weight_decay': 0.01,\n",
    "    'betas': (0.9, 0.999),\n",
    "\n",
    "    # Warmup\n",
    "    'warmup_epochs': 5,\n",
    "    'warmup_start_factor': 0.1,\n",
    "\n",
    "    # Scheduler\n",
    "    'scheduler': 'ReduceLROnPlateau',\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'scheduler_min_lr': 1e-7,\n",
    "\n",
    "    # Label smoothing & gradient clipping\n",
    "    'label_smoothing': 0.1,\n",
    "    'gradient_clip_norm': 1.0,\n",
    "\n",
    "    # Data augmentation\n",
    "    'horizontal_flip': 0.5,\n",
    "    'vertical_flip': 0.3,\n",
    "    'rotation_degrees': 20,\n",
    "    'translate': 0.15,\n",
    "    'scale': (0.85, 1.15),\n",
    "    'shear': 10,\n",
    "    'brightness': 0.2,\n",
    "    'contrast': 0.2,\n",
    "    'random_erasing_p': 0.2,\n",
    "\n",
    "    # Normalization (ImageNet)\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "\n",
    "    # Mixed Precision & Memory\n",
    "    'use_amp': True,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['metrics_dir'], exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Configuration saved to: {config_path}\")\n",
    "print(f\"\\nüìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üèóÔ∏è Base model: {CONFIG['pretrained_model']} (Hugging Face)\")\n",
    "print(f\"üì¶ Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"üî¢ Input size: {CONFIG['input_size']}x{CONFIG['input_size']} (SIN RESIZE)\")\n",
    "print(f\"üéØ Classes: {CONFIG['num_classes']} (Benign vs Malignant)\")\n",
    "print(f\"üîÑ Folds: {CONFIG['num_folds']}\")\n",
    "print(f\"üî• Warmup epochs: {CONFIG['warmup_epochs']}\")\n",
    "print(f\"‚ö° Mixed Precision: {CONFIG['use_amp']}\")\n",
    "print(f\"üè∑Ô∏è Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"‚úÇÔ∏è Gradient Clipping: {CONFIG['gradient_clip_norm']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(f\"‚úì Random seed set to {CONFIG['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_id(filename, dataset='ddsm'):\n",
    "    \"\"\"\n",
    "    Extract patient ID from filename to prevent data leakage.\n",
    "\n",
    "    DDSM format: P_00041_LEFT_CC_1.png -> P_00041\n",
    "    INbreast format: 20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png -> 20586908\n",
    "    \"\"\"\n",
    "    if dataset == 'ddsm':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}_{parts[1]}\"\n",
    "    elif dataset == 'inbreast':\n",
    "        return filename.split('_')[0]\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Test\n",
    "print(\"Testing patient ID extraction:\")\n",
    "print(f\"DDSM: P_00041_LEFT_CC_1.png -> {extract_patient_id('P_00041_LEFT_CC_1.png', 'ddsm')}\")\n",
    "print(f\"INbreast: 20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png -> {extract_patient_id('20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png', 'inbreast')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Custom dataset for mammography images (converts grayscale to RGB).\"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image as grayscale\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "\n",
    "        # Convert to RGB by duplicating the channel\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "print(\"‚úì Dataset class defined (with grayscale ‚Üí RGB conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=True):\n",
    "    \"\"\"Get data augmentation transforms - mantiene 512x512 original.\"\"\"\n",
    "\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            # SIN RESIZE - mantener 512x512\n",
    "            transforms.RandomHorizontalFlip(p=CONFIG['horizontal_flip']),\n",
    "            transforms.RandomVerticalFlip(p=CONFIG['vertical_flip']),\n",
    "            transforms.RandomRotation(degrees=CONFIG['rotation_degrees']),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=(CONFIG['translate'], CONFIG['translate']),\n",
    "                scale=CONFIG['scale'],\n",
    "                shear=CONFIG['shear']\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=CONFIG['brightness'],\n",
    "                contrast=CONFIG['contrast']\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std']),\n",
    "            transforms.RandomErasing(p=CONFIG['random_erasing_p'])\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            # SIN RESIZE - mantener 512x512\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "        ])\n",
    "\n",
    "    return transform\n",
    "\n",
    "print(\"‚úì Transform functions defined (mantiene 512x512 original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load all images from DDSM and INbreast datasets.\n",
    "    Returns: image_paths, labels, patient_ids\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    patient_ids = []\n",
    "\n",
    "    datasets = [\n",
    "        (CONFIG['ddsm_benign_path'], 0, 'ddsm'),\n",
    "        (CONFIG['ddsm_malign_path'], 1, 'ddsm'),\n",
    "        (CONFIG['inbreast_benign_path'], 0, 'inbreast'),\n",
    "        (CONFIG['inbreast_malign_path'], 1, 'inbreast')\n",
    "    ]\n",
    "\n",
    "    for path, label, dataset_name in datasets:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è WARNING: Path not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        for filename in files:\n",
    "            img_path = os.path.join(path, filename)\n",
    "            patient_id = extract_patient_id(filename, dataset_name)\n",
    "\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(label)\n",
    "            patient_ids.append(patient_id)\n",
    "\n",
    "    return image_paths, labels, patient_ids\n",
    "\n",
    "# Load data\n",
    "print(\"Loading dataset...\")\n",
    "image_paths, labels, patient_ids = load_dataset()\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded:\")\n",
    "print(f\"  Total images: {len(image_paths)}\")\n",
    "print(f\"  Benign: {labels.count(0)}\")\n",
    "print(f\"  Malignant: {labels.count(1)}\")\n",
    "print(f\"  Unique patients: {len(set(patient_ids))}\")\n",
    "\n",
    "# Check class balance\n",
    "class_counts = pd.Series(labels).value_counts()\n",
    "print(f\"\\n‚öñÔ∏è Class distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    percentage = (count / len(labels)) * 100\n",
    "    cls_name = \"Benign\" if cls == 0 else \"Malignant\"\n",
    "    print(f\"  {cls_name}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvTForImageClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    CvT model for binary classification using Hugging Face.\n",
    "    Adaptado para im√°genes 512x512.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='microsoft/cvt-13', num_classes=2, pretrained=True, image_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Cargar config y modificar image_size\n",
    "        config = CvtConfig.from_pretrained(model_name)\n",
    "        config.image_size = image_size\n",
    "\n",
    "        if pretrained:\n",
    "            print(f\"Loading pre-trained CvT from {model_name}...\")\n",
    "            print(f\"‚ö†Ô∏è Adaptando de 224x224 a {image_size}x{image_size}\")\n",
    "            # ignore_mismatched_sizes permite usar pesos pre-entrenados con diferente tama√±o\n",
    "            self.cvt = CvtModel.from_pretrained(\n",
    "                model_name,\n",
    "                config=config,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "        else:\n",
    "            self.cvt = CvtModel(config)\n",
    "\n",
    "        # Get hidden size from last stage\n",
    "        self.hidden_size = self.cvt.config.embed_dim[-1]\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "        print(f\"‚úì Model created with {self.hidden_size} hidden size\")\n",
    "        print(f\"‚úì Input size: {image_size}x{image_size}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Get CvT features\n",
    "        outputs = self.cvt(pixel_values=pixel_values)\n",
    "\n",
    "        # Use [CLS] token (first token)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(cls_token)\n",
    "\n",
    "        return logits\n",
    "\n",
    "print(\"‚úì CvTForImageClassification class defined (soporte para 512x512)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(pretrained=True, num_classes=2, freeze_strategy='none'):\n",
    "    \"\"\"\n",
    "    Create CvT model from Hugging Face with 512x512 support.\n",
    "    \"\"\"\n",
    "\n",
    "    model = CvTForImageClassification(\n",
    "        model_name=CONFIG['pretrained_model'],\n",
    "        num_classes=num_classes,\n",
    "        pretrained=pretrained,\n",
    "        image_size=CONFIG['input_size']\n",
    "    )\n",
    "\n",
    "    # Apply freezing strategy\n",
    "    if freeze_strategy == 'all_except_head':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"  ‚ùÑÔ∏è Frozen all layers except head\")\n",
    "\n",
    "    elif freeze_strategy == 'progressive':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"  ‚ùÑÔ∏è Started with frozen backbone (progressive unfreezing enabled)\")\n",
    "\n",
    "    elif freeze_strategy == 'none':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"  üî• All layers unfrozen (full fine-tuning)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "print(\"‚úì create_model function defined (512x512)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvTForImageClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    CvT model for binary classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='microsoft/cvt-13', num_classes=2, pretrained=True, image_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        config = CvtConfig.from_pretrained(model_name)\n",
    "\n",
    "        if pretrained:\n",
    "            print(f\"Loading pre-trained CvT from {model_name}...\")\n",
    "            self.cvt = CvtModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.cvt = CvtModel(config)\n",
    "\n",
    "        # Hidden size del √∫ltimo stage\n",
    "        self.hidden_size = config.embed_dim[-1]  # 384 para cvt-13\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Clasificador simple\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "        print(f\"‚úì Model created with {self.hidden_size} hidden size\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Get features\n",
    "        outputs = self.cvt(pixel_values=pixel_values, return_dict=True)\n",
    "\n",
    "        # last_hidden_state: [batch, seq_len, hidden_size]\n",
    "        features = outputs.last_hidden_state\n",
    "\n",
    "        # Debug: ver el shape real\n",
    "        # print(f\"DEBUG - features shape: {features.shape}\")\n",
    "\n",
    "        # Reshape si es necesario\n",
    "        batch_size = features.shape[0]\n",
    "\n",
    "        # Flatten y luego pooling si tiene m√°s dimensiones\n",
    "        if len(features.shape) == 4:\n",
    "            # [batch, channels, H, W] -> [batch, channels]\n",
    "            pooled = features.mean(dim=[2, 3])\n",
    "        elif len(features.shape) == 3:\n",
    "            # [batch, seq_len, hidden_size] -> [batch, hidden_size]\n",
    "            pooled = features.mean(dim=1)\n",
    "        else:\n",
    "            # Ya est√° en el formato correcto\n",
    "            pooled = features\n",
    "\n",
    "        # Asegurar que tenga el shape correcto [batch, hidden_size]\n",
    "        if pooled.shape[-1] != self.hidden_size:\n",
    "            # Necesitamos hacer m√°s pooling\n",
    "            pooled = pooled.reshape(batch_size, -1)\n",
    "            # Proyectar al hidden_size correcto\n",
    "            if not hasattr(self, 'projection'):\n",
    "                self.projection = nn.Linear(pooled.shape[-1], self.hidden_size).to(pooled.device)\n",
    "            pooled = self.projection(pooled)\n",
    "\n",
    "        # Dropout\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Classification: [batch, num_classes]\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        return logits\n",
    "\n",
    "print(\"‚úì CvTForImageClassification class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    \"\"\"Learning Rate Finder using the LR Range Test.\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "        # Save initial state\n",
    "        self.model_state = model.state_dict()\n",
    "        self.optimizer_state = optimizer.state_dict()\n",
    "\n",
    "    def range_test(self, train_loader, start_lr=1e-7, end_lr=1e-3, num_iter=100, smooth_f=0.05):\n",
    "        \"\"\"Perform LR range test.\"\"\"\n",
    "\n",
    "        # Reset model and optimizer\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "\n",
    "        # Calculate LR multiplier\n",
    "        mult = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        avg_loss = 0.\n",
    "        best_loss = float('inf')\n",
    "        batch_num = 0\n",
    "        losses = []\n",
    "        lrs = []\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        iterator = iter(train_loader)\n",
    "\n",
    "        for iteration in tqdm(range(num_iter), desc=\"LR Finder\"):\n",
    "            # Get batch\n",
    "            try:\n",
    "                inputs, targets = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(train_loader)\n",
    "                inputs, targets = next(iterator)\n",
    "\n",
    "            batch_num += 1\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "            # Forward\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            # Compute smoothed loss\n",
    "            avg_loss = smooth_f * loss.item() + (1 - smooth_f) * avg_loss\n",
    "            smoothed_loss = avg_loss / (1 - (1 - smooth_f) ** batch_num)\n",
    "\n",
    "            # Stop if loss explodes\n",
    "            if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "                print(f\"\\n‚ö†Ô∏è Loss exploded at LR={lr:.2e}\")\n",
    "                break\n",
    "\n",
    "            # Record best loss\n",
    "            if smoothed_loss < best_loss or batch_num == 1:\n",
    "                best_loss = smoothed_loss\n",
    "\n",
    "            # Store values\n",
    "            losses.append(smoothed_loss)\n",
    "            lrs.append(lr)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update LR\n",
    "            lr *= mult\n",
    "            self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        # Reset model and optimizer\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "\n",
    "        return lrs, losses\n",
    "\n",
    "    def plot(self, lrs, losses, skip_start=10, skip_end=5):\n",
    "        \"\"\"Plot LR range test results.\"\"\"\n",
    "\n",
    "        if skip_start >= len(lrs):\n",
    "            skip_start = 0\n",
    "        if skip_end >= len(lrs):\n",
    "            skip_end = 0\n",
    "\n",
    "        lrs = lrs[skip_start:-skip_end] if skip_end > 0 else lrs[skip_start:]\n",
    "        losses = losses[skip_start:-skip_end] if skip_end > 0 else losses[skip_start:]\n",
    "\n",
    "        # Find minimum\n",
    "        min_idx = np.argmin(losses)\n",
    "        min_lr = lrs[min_idx]\n",
    "\n",
    "        # Suggested LR (10x smaller than minimum for fine-tuning)\n",
    "        suggested_lr = min_lr / 10\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder (Transfer Learning)')\n",
    "        plt.axvline(x=min_lr, color='r', linestyle='--', label=f'Min Loss LR: {min_lr:.2e}')\n",
    "        plt.axvline(x=suggested_lr, color='g', linestyle='--', label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Save plot\n",
    "        plot_path = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_lr_finder.png\")\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nüìä LR Finder Results (Transfer Learning):\")\n",
    "        print(f\"  Minimum loss LR: {min_lr:.2e}\")\n",
    "        print(f\"  Suggested LR: {suggested_lr:.2e}\")\n",
    "        print(f\"  Plot saved to: {plot_path}\")\n",
    "\n",
    "        return suggested_lr\n",
    "\n",
    "print(\"‚úì LRFinder class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LR Finder\n",
    "print(\"\\nüîç Running Learning Rate Finder for Transfer Learning...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Create temporary model\n",
    "temp_model = create_model(\n",
    "    pretrained=CONFIG['pretrained'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    freeze_strategy=CONFIG['freeze_strategy']\n",
    ").to(device)\n",
    "\n",
    "# Create temporary dataset (use first 100 images for speed)\n",
    "temp_indices = list(range(min(100, len(image_paths))))  # Reducido a 100\n",
    "temp_paths = [image_paths[i] for i in temp_indices]\n",
    "temp_labels = [labels[i] for i in temp_indices]\n",
    "\n",
    "print(f\"Temp dataset: {len(temp_paths)} images\")\n",
    "print(f\"Label distribution: Benign={temp_labels.count(0)}, Malignant={temp_labels.count(1)}\")\n",
    "\n",
    "temp_dataset = MammographyDataset(\n",
    "    temp_paths,\n",
    "    temp_labels,\n",
    "    transform=get_transforms(train=True)\n",
    ")\n",
    "\n",
    "temp_loader = DataLoader(\n",
    "    temp_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Reducido a 0 para debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Test one batch\n",
    "print(\"\\nüß™ Testing one batch...\")\n",
    "try:\n",
    "    test_batch = next(iter(temp_loader))\n",
    "    test_images, test_labels = test_batch\n",
    "    print(f\"  Batch images shape: {test_images.shape}\")\n",
    "    print(f\"  Batch labels shape: {test_labels.shape}\")\n",
    "    print(f\"  Labels dtype: {test_labels.dtype}\")\n",
    "    print(f\"  Labels values: {test_labels}\")\n",
    "\n",
    "    # Test forward pass\n",
    "    test_images = test_images.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = temp_model(test_images)\n",
    "        print(f\"  Model output shape: {test_output.shape}\")\n",
    "\n",
    "        # Test loss\n",
    "        temp_criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "        test_loss = temp_criterion(test_output, test_labels)\n",
    "        print(f\"  ‚úì Loss computation works: {test_loss.item():.4f}\")\n",
    "\n",
    "    del test_images, test_labels, test_output, test_loss\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Batch test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Clean up and exit\n",
    "    del temp_model, temp_dataset, temp_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise\n",
    "\n",
    "# Setup for LR Finder\n",
    "temp_optimizer = AdamW(\n",
    "    temp_model.parameters(),\n",
    "    lr=1e-7,\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    betas=CONFIG['betas']\n",
    ")\n",
    "\n",
    "temp_criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "print(\"\\nüöÄ Starting LR Finder...\")\n",
    "\n",
    "# Run LR Finder\n",
    "try:\n",
    "    lr_finder = LRFinder(temp_model, temp_optimizer, temp_criterion, device)\n",
    "    lrs, losses = lr_finder.range_test(\n",
    "        temp_loader,\n",
    "        start_lr=CONFIG['lr_min'],\n",
    "        end_lr=CONFIG['lr_max'],\n",
    "        num_iter=50  # Reducido a 50 para que sea m√°s r√°pido\n",
    "    )\n",
    "\n",
    "    # Plot and get suggested LR\n",
    "    suggested_lr = lr_finder.plot(lrs, losses)\n",
    "\n",
    "    # Update config\n",
    "    CONFIG['lr_initial'] = suggested_lr\n",
    "\n",
    "    print(f\"\\n‚úì Learning rate set to: {CONFIG['lr_initial']:.2e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå LR Finder failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Set default LR\n",
    "    CONFIG['lr_initial'] = 1e-4\n",
    "    print(f\"\\n‚ö†Ô∏è Using default LR: {CONFIG['lr_initial']:.2e}\")\n",
    "\n",
    "# Clean up\n",
    "del temp_model, temp_dataset, temp_loader, temp_optimizer, temp_criterion\n",
    "if 'lr_finder' in locals():\n",
    "    del lr_finder\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì LR Finder complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=15, min_delta=1e-4, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, epoch):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"  EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "\n",
    "print(\"‚úì EarlyStopping class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, device, use_amp=True, clip_norm=None):\n",
    "    \"\"\"Train for one epoch with gradient clipping.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for inputs, targets in pbar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if clip_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validation\")\n",
    "        for inputs, targets in pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_preds, all_targets\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate classification metrics.\"\"\"\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Specificity\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    else:\n",
    "        specificity = 0\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'specificity': specificity,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cross_validation():\n",
    "    \"\"\"\n",
    "    Train model with 10-fold cross validation and warmup scheduler.\n",
    "    Split by patient to prevent data leakage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group by patient\n",
    "    patient_to_indices = defaultdict(list)\n",
    "    patient_to_label = {}\n",
    "\n",
    "    for idx, (patient_id, label) in enumerate(zip(patient_ids, labels)):\n",
    "        patient_to_indices[patient_id].append(idx)\n",
    "        patient_to_label[patient_id] = label\n",
    "\n",
    "    unique_patients = list(patient_to_indices.keys())\n",
    "    patient_labels = [patient_to_label[p] for p in unique_patients]\n",
    "\n",
    "    print(f\"\\nüìä Cross Validation Setup:\")\n",
    "    print(f\"  Total patients: {len(unique_patients)}\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Folds: {CONFIG['num_folds']}\")\n",
    "\n",
    "    # K-Fold split\n",
    "    skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "    # Store results\n",
    "    fold_metrics = []\n",
    "    fold_histories = []\n",
    "    fold_cms = []\n",
    "\n",
    "    # Train each fold\n",
    "    for fold, (train_patient_idx, val_patient_idx) in enumerate(skf.split(unique_patients, patient_labels)):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Get patient IDs for this fold\n",
    "        train_patients = [unique_patients[i] for i in train_patient_idx]\n",
    "        val_patients = [unique_patients[i] for i in val_patient_idx]\n",
    "\n",
    "        # Get image indices\n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "\n",
    "        for patient in train_patients:\n",
    "            train_indices.extend(patient_to_indices[patient])\n",
    "\n",
    "        for patient in val_patients:\n",
    "            val_indices.extend(patient_to_indices[patient])\n",
    "\n",
    "        # Create datasets\n",
    "        train_paths = [image_paths[i] for i in train_indices]\n",
    "        train_labels = [labels[i] for i in train_indices]\n",
    "        val_paths = [image_paths[i] for i in val_indices]\n",
    "        val_labels = [labels[i] for i in val_indices]\n",
    "\n",
    "        train_dataset = MammographyDataset(train_paths, train_labels, transform=get_transforms(train=True))\n",
    "        val_dataset = MammographyDataset(val_paths, val_labels, transform=get_transforms(train=False))\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory']\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory']\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüìä Fold {fold + 1} Data:\")\n",
    "        print(f\"  Train patients: {len(train_patients)}\")\n",
    "        print(f\"  Val patients: {len(val_patients)}\")\n",
    "        print(f\"  Train images: {len(train_paths)} (Benign: {train_labels.count(0)}, Malignant: {train_labels.count(1)})\")\n",
    "        print(f\"  Val images: {len(val_paths)} (Benign: {val_labels.count(0)}, Malignant: {val_labels.count(1)})\")\n",
    "\n",
    "        # Create model\n",
    "        model = create_model(\n",
    "            pretrained=CONFIG['pretrained'],\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            freeze_strategy=CONFIG['freeze_strategy']\n",
    "        ).to(device)\n",
    "\n",
    "        # Loss with label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=CONFIG['lr_initial'],\n",
    "            weight_decay=CONFIG['weight_decay'],\n",
    "            betas=CONFIG['betas']\n",
    "        )\n",
    "\n",
    "        # Warmup + ReduceLROnPlateau scheduler\n",
    "        warmup_scheduler = LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=CONFIG['warmup_start_factor'],\n",
    "            total_iters=CONFIG['warmup_epochs']\n",
    "        )\n",
    "\n",
    "        main_scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=CONFIG['scheduler_factor'],\n",
    "            patience=CONFIG['scheduler_patience'],\n",
    "            min_lr=CONFIG['scheduler_min_lr'],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=CONFIG['early_stopping_patience'],\n",
    "            min_delta=CONFIG['min_delta'],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Mixed precision scaler\n",
    "        scaler = GradScaler() if CONFIG['use_amp'] else None\n",
    "\n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "        # Training loop\n",
    "        print(f\"\\nüöÄ Starting training with warmup ({CONFIG['warmup_epochs']} epochs)...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            print(f\"\\n--- Epoch {epoch + 1}/{CONFIG['num_epochs']} ---\")\n",
    "\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, scaler, device,\n",
    "                CONFIG['use_amp'], CONFIG['gradient_clip_norm']\n",
    "            )\n",
    "\n",
    "            # Validate\n",
    "            val_loss, val_acc, val_preds, val_targets = validate_epoch(\n",
    "                model, val_loader, criterion, device\n",
    "            )\n",
    "\n",
    "            # Update scheduler\n",
    "            if epoch < CONFIG['warmup_epochs']:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                main_scheduler.step(val_loss)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['lr'].append(current_lr)\n",
    "\n",
    "            # Print epoch summary\n",
    "            phase = \"Warmup\" if epoch < CONFIG['warmup_epochs'] else \"Main\"\n",
    "            print(f\"\\nüìä Epoch {epoch + 1} Summary ({phase}):\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "            print(f\"  LR: {current_lr:.2e}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "                model_path = os.path.join(\n",
    "                    CONFIG['output_dir'],\n",
    "                    f\"{CONFIG['model_name']}_fold{fold}.pth\"\n",
    "                )\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                    'config': CONFIG\n",
    "                }, model_path)\n",
    "\n",
    "                print(f\"  ‚úì Model saved: {model_path}\")\n",
    "\n",
    "            # Early stopping (only after warmup)\n",
    "            if epoch >= CONFIG['warmup_epochs']:\n",
    "                early_stopping(val_loss, epoch + 1)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    print(f\"  Best epoch: {best_epoch} with val_loss: {best_val_loss:.4f}\")\n",
    "                    break\n",
    "\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\n‚è±Ô∏è Training time for fold {fold + 1}: {training_time/60:.2f} minutes\")\n",
    "\n",
    "        # Load best model for evaluation\n",
    "        model_path = os.path.join(CONFIG['output_dir'], f\"{CONFIG['model_name']}_fold{fold}.pth\")\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Final evaluation\n",
    "        print(f\"\\nüìä Final evaluation on validation set...\")\n",
    "        _, _, final_preds, final_targets = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(final_targets, final_preds)\n",
    "\n",
    "        print(f\"\\n‚úÖ Fold {fold + 1} Results:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  Precision: {metrics['precision']*100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall']*100:.2f}%\")\n",
    "        print(f\"  F1-Score: {metrics['f1']*100:.2f}%\")\n",
    "        print(f\"  Specificity: {metrics['specificity']*100:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        fold_metrics.append(metrics)\n",
    "        fold_histories.append(history)\n",
    "        fold_cms.append(metrics['confusion_matrix'])\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, fold)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(metrics['confusion_matrix'], fold)\n",
    "\n",
    "        # Clean up\n",
    "        del model, optimizer, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return fold_metrics, fold_histories, fold_cms\n",
    "\n",
    "print(\"‚úì train_with_cross_validation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, fold):\n",
    "    \"\"\"Plot training history with warmup indicator.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'Fold {fold + 1}: Loss (Transfer Learning)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[1].plot([acc*100 for acc in history['train_acc']], label='Train Acc', marker='o')\n",
    "    axes[1].plot([acc*100 for acc in history['val_acc']], label='Val Acc', marker='s')\n",
    "    axes[1].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title(f'Fold {fold + 1}: Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning Rate\n",
    "    axes[2].plot(history['lr'], marker='o')\n",
    "    axes[2].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title(f'Fold {fold + 1}: Learning Rate (with Warmup)')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_fold{fold}_history.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"  History plot saved: {plot_path}\")\n",
    "\n",
    "def plot_confusion_matrix(cm, fold, normalize=False):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='.2f' if normalize else 'd',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(f'Fold {fold + 1}: Confusion Matrix')\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_fold{fold}_cm.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"  Confusion matrix saved: {plot_path}\")\n",
    "\n",
    "def save_metrics_summary(fold_metrics):\n",
    "    \"\"\"Save metrics summary to CSV.\"\"\"\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Fold': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Specificity': []\n",
    "    }\n",
    "\n",
    "    for fold, metrics in enumerate(fold_metrics):\n",
    "        metrics_dict['Fold'].append(fold + 1)\n",
    "        metrics_dict['Accuracy'].append(metrics['accuracy'])\n",
    "        metrics_dict['Precision'].append(metrics['precision'])\n",
    "        metrics_dict['Recall'].append(metrics['recall'])\n",
    "        metrics_dict['F1-Score'].append(metrics['f1'])\n",
    "        metrics_dict['Specificity'].append(metrics['specificity'])\n",
    "\n",
    "    # Calculate mean and std\n",
    "    for key in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']:\n",
    "        values = metrics_dict[key]\n",
    "        metrics_dict['Fold'].append('Mean ¬± Std')\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        metrics_dict[key].append(f\"{mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "        break\n",
    "\n",
    "    for key in ['Precision', 'Recall', 'F1-Score', 'Specificity']:\n",
    "        values = [m for m in metrics_dict[key] if isinstance(m, float)]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        metrics_dict[key].append(f\"{mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "    df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "    csv_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    )\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Metrics saved to: {csv_path}\")\n",
    "    print(f\"\\n{df.to_string(index=False)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_average_confusion_matrix(fold_cms):\n",
    "    \"\"\"Plot average confusion matrix across all folds.\"\"\"\n",
    "\n",
    "    avg_cm = np.mean(fold_cms, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        avg_cm,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Average Confusion Matrix (10-Fold CV - Transfer Learning)')\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_avg_cm.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n‚úÖ Average confusion matrix saved: {plot_path}\")\n",
    "\n",
    "print(\"‚úì Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING 10-FOLD CROSS VALIDATION TRAINING (TRANSFER LEARNING - HUGGING FACE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "fold_metrics, fold_histories, fold_cms = train_with_cross_validation()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚è±Ô∏è Total training time: {total_time/3600:.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
