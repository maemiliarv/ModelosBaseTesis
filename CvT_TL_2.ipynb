{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Instalar Hugging Face Transformers\n",
    "!pip install transformers datasets pillow -q\n",
    "\n",
    "print(\"‚úì Transformers instalado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from transformers import CvtConfig, CvtModel\n",
    "\n",
    "print(\"Verificando modelos CvT en Hugging Face...\\n\")\n",
    "\n",
    "cvt_models = [\n",
    "    'microsoft/cvt-13',    # 20M params - RECOMENDADO\n",
    "    'microsoft/cvt-21',    # 32M params\n",
    "    'microsoft/cvt-w24'    # 277M params - MUY GRANDE\n",
    "]\n",
    "\n",
    "for model_name in cvt_models:\n",
    "    try:\n",
    "        config = CvtConfig.from_pretrained(model_name)\n",
    "        print(f\"‚úì {model_name}\")\n",
    "        print(f\"  Depth: {config.depth}\")\n",
    "        print(f\"  Embed dims: {config.embed_dim}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {model_name} - Error\\n\")\n",
    "\n",
    "print(\"üí° Recomendado: microsoft/cvt-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import CvtModel, CvtConfig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports loaded successfully (CvT_TL_2 with Advanced Techniques)\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# GPU verification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úì GPU cache cleared\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU available, training will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Configuration para CvT_TL_2 (512x512) - MEJORADO\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'base_path': '/home/merivadeneira',\n",
    "    'ddsm_benign_path': '/home/merivadeneira/Masas/DDSM/Benignas/Resized_512',\n",
    "    'ddsm_malign_path': '/home/merivadeneira/Masas/DDSM/Malignas/Resized_512',\n",
    "    'inbreast_benign_path': '/home/merivadeneira/Masas/INbreast/Benignas/Resized_512',\n",
    "    'inbreast_malign_path': '/home/merivadeneira/Masas/INbreast/Malignas/Resized_512',\n",
    "    'output_dir': '/home/merivadeneira/Outputs/CvT',\n",
    "    'metrics_dir': '/home/merivadeneira/Metrics/CvT',\n",
    "\n",
    "    # Model (HUGGING FACE)\n",
    "    'model_name': 'CvT_TL_2',\n",
    "    'pretrained_model': 'microsoft/cvt-13',\n",
    "    'pretrained': True,\n",
    "    'input_size': 512,\n",
    "    'in_channels': 3,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Fine-tuning strategy\n",
    "    'freeze_strategy': 'none',\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 100,\n",
    "    'num_folds': 5,\n",
    "    'early_stopping_patience': 25,\n",
    "    'min_delta': 1e-4,\n",
    "    'gradient_accumulation_steps': 2,  # NUEVO: batch efectivo = 32\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr_initial': None,\n",
    "    'lr_min': 1e-7,\n",
    "    'lr_max': 1e-3,\n",
    "    'weight_decay': 0.01,\n",
    "    'betas': (0.9, 0.999),\n",
    "\n",
    "    # Warmup\n",
    "    'warmup_epochs': 5,\n",
    "    'warmup_start_factor': 0.1,\n",
    "\n",
    "    # Scheduler - NUEVO: CosineAnnealingWarmRestarts\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'cosine_T0': 10,        # Reinicia cada 10 √©pocas\n",
    "    'cosine_T_mult': 2,     # Duplica el periodo\n",
    "    'cosine_eta_min': 1e-7,\n",
    "\n",
    "    # Focal Loss - NUEVO\n",
    "    'use_focal_loss': True,\n",
    "    'focal_alpha': 0.25,    # Peso para clase positiva (maligno)\n",
    "    'focal_gamma': 2.0,     # Factor de enfoque\n",
    "\n",
    "    # Class Weights - NUEVO\n",
    "    'use_class_weights': True,\n",
    "\n",
    "    # Mixup - NUEVO\n",
    "    'use_mixup': True,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'mixup_prob': 0.5,      # 50% de batches con mixup\n",
    "\n",
    "    # SWA - NUEVO\n",
    "    'use_swa': True,\n",
    "    'swa_start_epoch': 70,  # Empezar SWA despu√©s del 70% del entrenamiento\n",
    "    'swa_lr': 1e-5,\n",
    "\n",
    "    # TTA - NUEVO\n",
    "    'use_tta': True,\n",
    "    'tta_augmentations': 5,\n",
    "\n",
    "    # Label smoothing & gradient clipping\n",
    "    'label_smoothing': 0.1,\n",
    "    'gradient_clip_norm': 1.0,\n",
    "\n",
    "    # Data augmentation - MEJORADO (SIN SCALE)\n",
    "    'horizontal_flip': 0.5,\n",
    "    'vertical_flip': 0.3,\n",
    "    'rotation_degrees': 30,        # 20 ‚Üí 30\n",
    "    'translate': 0.20,             # 0.15 ‚Üí 0.20\n",
    "    'shear': 10,\n",
    "    'brightness': 0.3,             # 0.2 ‚Üí 0.3\n",
    "    'contrast': 0.3,               # 0.2 ‚Üí 0.3\n",
    "    'random_erasing_p': 0.3,       # 0.2 ‚Üí 0.3\n",
    "\n",
    "    # Normalization (ImageNet)\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "\n",
    "    # Mixed Precision & Memory\n",
    "    'use_amp': True,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['metrics_dir'], exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Configuration saved to: {config_path}\")\n",
    "print(f\"\\nüìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üèóÔ∏è Base model: {CONFIG['pretrained_model']} (Hugging Face)\")\n",
    "print(f\"üì¶ Batch size: {CONFIG['batch_size']} (effective: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"üî¢ Input size: {CONFIG['input_size']}x{CONFIG['input_size']}\")\n",
    "print(f\"üéØ Classes: {CONFIG['num_classes']} (Benign vs Malignant)\")\n",
    "print(f\"üîÑ Folds: {CONFIG['num_folds']}\")\n",
    "print(f\"üî• Warmup epochs: {CONFIG['warmup_epochs']}\")\n",
    "print(f\"‚ö° Mixed Precision: {CONFIG['use_amp']}\")\n",
    "print(f\"üéØ Focal Loss: {CONFIG['use_focal_loss']} (alpha={CONFIG['focal_alpha']}, gamma={CONFIG['focal_gamma']})\")\n",
    "print(f\"‚öñÔ∏è Class Weights: {CONFIG['use_class_weights']}\")\n",
    "print(f\"üé® Mixup: {CONFIG['use_mixup']} (alpha={CONFIG['mixup_alpha']})\")\n",
    "print(f\"üìä SWA: {CONFIG['use_swa']} (start epoch={CONFIG['swa_start_epoch']})\")\n",
    "print(f\"üîç TTA: {CONFIG['use_tta']} (augmentations={CONFIG['tta_augmentations']})\")\n",
    "print(f\"üîÑ Gradient Accumulation: {CONFIG['gradient_accumulation_steps']} steps\")\n",
    "print(f\"üìà Scheduler: {CONFIG['scheduler']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(f\"‚úì Random seed set to {CONFIG['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_patient_id(filename, dataset='ddsm'):\n",
    "    \"\"\"\n",
    "    Extract patient ID from filename to prevent data leakage.\n",
    "\n",
    "    DDSM format: P_00041_LEFT_CC_1.png -> P_00041\n",
    "    INbreast format: 20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png -> 20586908\n",
    "    \"\"\"\n",
    "    if dataset == 'ddsm':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}_{parts[1]}\"\n",
    "    elif dataset == 'inbreast':\n",
    "        return filename.split('_')[0]\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Test\n",
    "print(\"Testing patient ID extraction:\")\n",
    "print(f\"DDSM: P_00041_LEFT_CC_1.png -> {extract_patient_id('P_00041_LEFT_CC_1.png', 'ddsm')}\")\n",
    "print(f\"INbreast: 20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png -> {extract_patient_id('20586908_6c613a14b80a8591_MG_R_CC_ANON_lesion1_ROI.png', 'inbreast')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# NUEVO: Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss para manejar desbalance de clases y enfocarse en ejemplos dif√≠ciles.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, label_smoothing=0.1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Aplicar label smoothing\n",
    "        num_classes = inputs.size(-1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).float()\n",
    "        \n",
    "        if self.label_smoothing > 0:\n",
    "            targets_one_hot = targets_one_hot * (1 - self.label_smoothing) + \\\n",
    "                             self.label_smoothing / num_classes\n",
    "\n",
    "        # Calcular probabilidades\n",
    "        p = F.softmax(inputs, dim=-1)\n",
    "        ce_loss = -targets_one_hot * torch.log(p + 1e-8)\n",
    "\n",
    "        # Aplicar focal loss\n",
    "        p_t = (targets_one_hot * p).sum(dim=-1)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Aplicar alpha weight\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            focal_loss = alpha_t * focal_weight * ce_loss.sum(dim=-1)\n",
    "        else:\n",
    "            focal_loss = focal_weight * ce_loss.sum(dim=-1)\n",
    "\n",
    "        return focal_loss.mean()\n",
    "\n",
    "print(\"‚úì Focal Loss implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# NUEVO: Mixup Implementation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Mixup augmentation: mezcla pares de im√°genes y sus labels.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"\n",
    "    Calcula loss para mixup.\n",
    "    \"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "print(\"‚úì Mixup implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Custom dataset for mammography images (converts grayscale to RGB).\"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image as grayscale\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "\n",
    "        # Convert to RGB by duplicating the channel\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "print(\"‚úì Dataset class defined (with grayscale ‚Üí RGB conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_transforms(train=True):\n",
    "    \"\"\"Get data augmentation transforms - MEJORADO (SIN SCALE).\"\"\"\n",
    "\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            # NO RESIZE - mantiene 512x512 original\n",
    "            transforms.RandomHorizontalFlip(p=CONFIG['horizontal_flip']),\n",
    "            transforms.RandomVerticalFlip(p=CONFIG['vertical_flip']),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=CONFIG['rotation_degrees'],\n",
    "                translate=(CONFIG['translate'], CONFIG['translate']),\n",
    "                shear=CONFIG['shear'],\n",
    "                # SIN SCALE - removido\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=CONFIG['brightness'],\n",
    "                contrast=CONFIG['contrast']\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=CONFIG['mean'],\n",
    "                std=CONFIG['std']\n",
    "            ),\n",
    "            transforms.RandomErasing(\n",
    "                p=CONFIG['random_erasing_p'],\n",
    "                scale=(0.02, 0.33),\n",
    "                ratio=(0.3, 3.3)\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            # NO RESIZE - mantiene 512x512 original\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=CONFIG['mean'],\n",
    "                std=CONFIG['std']\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    return transform\n",
    "\n",
    "print(\"‚úì Data augmentation transforms defined (IMPROVED - NO SCALE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_dataset():\n",
    "    \"\"\"Load mammography dataset from all sources.\"\"\"\n",
    "\n",
    "    print(\"\\nüìÇ Loading dataset...\")\n",
    "\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    patient_ids = []\n",
    "\n",
    "    # DDSM Benign\n",
    "    ddsm_benign_files = sorted(os.listdir(CONFIG['ddsm_benign_path']))\n",
    "    for filename in ddsm_benign_files:\n",
    "        if filename.endswith('.png'):\n",
    "            image_paths.append(os.path.join(CONFIG['ddsm_benign_path'], filename))\n",
    "            labels.append(0)\n",
    "            patient_ids.append(extract_patient_id(filename, 'ddsm'))\n",
    "\n",
    "    # DDSM Malignant\n",
    "    ddsm_malign_files = sorted(os.listdir(CONFIG['ddsm_malign_path']))\n",
    "    for filename in ddsm_malign_files:\n",
    "        if filename.endswith('.png'):\n",
    "            image_paths.append(os.path.join(CONFIG['ddsm_malign_path'], filename))\n",
    "            labels.append(1)\n",
    "            patient_ids.append(extract_patient_id(filename, 'ddsm'))\n",
    "\n",
    "    # INbreast Benign\n",
    "    inbreast_benign_files = sorted(os.listdir(CONFIG['inbreast_benign_path']))\n",
    "    for filename in inbreast_benign_files:\n",
    "        if filename.endswith('.png'):\n",
    "            image_paths.append(os.path.join(CONFIG['inbreast_benign_path'], filename))\n",
    "            labels.append(0)\n",
    "            patient_ids.append(extract_patient_id(filename, 'inbreast'))\n",
    "\n",
    "    # INbreast Malignant\n",
    "    inbreast_malign_files = sorted(os.listdir(CONFIG['inbreast_malign_path']))\n",
    "    for filename in inbreast_malign_files:\n",
    "        if filename.endswith('.png'):\n",
    "            image_paths.append(os.path.join(CONFIG['inbreast_malign_path'], filename))\n",
    "            labels.append(1)\n",
    "            patient_ids.append(extract_patient_id(filename, 'inbreast'))\n",
    "\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    patient_ids = np.array(patient_ids)\n",
    "\n",
    "    print(f\"\\n‚úì Dataset loaded:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Benign (0): {np.sum(labels == 0)} ({np.sum(labels == 0)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"  Malignant (1): {np.sum(labels == 1)} ({np.sum(labels == 1)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"  Unique patients: {len(np.unique(patient_ids))}\")\n",
    "\n",
    "    return image_paths, labels, patient_ids\n",
    "\n",
    "# Load dataset\n",
    "image_paths, labels, patient_ids = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# NUEVO: Calculate class weights\n",
    "def calculate_class_weights(labels):\n",
    "    \"\"\"Calculate class weights for imbalanced dataset.\"\"\"\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = len(labels) / (len(class_counts) * class_counts)\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Class weights calculated:\")\n",
    "    print(f\"  Benign (0): {class_weights[0]:.4f}\")\n",
    "    print(f\"  Malignant (1): {class_weights[1]:.4f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "if CONFIG['use_class_weights']:\n",
    "    class_weights = calculate_class_weights(labels)\n",
    "else:\n",
    "    class_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CvTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CvT-based classifier for mammography using Hugging Face.\n",
    "    Mejorado con Dropout m√°s alto.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='microsoft/cvt-13', num_classes=2, pretrained=True):\n",
    "        super(CvTClassifier, self).__init__()\n",
    "\n",
    "        # Load pretrained CvT model from Hugging Face\n",
    "        if pretrained:\n",
    "            self.cvt = CvtModel.from_pretrained(model_name)\n",
    "            print(f\"‚úì Loaded pretrained weights from {model_name}\")\n",
    "        else:\n",
    "            config = CvtConfig.from_pretrained(model_name)\n",
    "            self.cvt = CvtModel(config)\n",
    "            print(f\"‚úì Initialized CvT from config (no pretrained weights)\")\n",
    "\n",
    "        # Get feature dimension\n",
    "        config = self.cvt.config\n",
    "        self.feature_dim = config.embed_dim[-1]\n",
    "\n",
    "        # Classification head con Dropout aumentado\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),  # 0.3 ‚Üí 0.5\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize classifier\n",
    "        self._init_classifier()\n",
    "\n",
    "    def _init_classifier(self):\n",
    "        \"\"\"Initialize classifier weights.\"\"\"\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CvT forward\n",
    "        outputs = self.cvt(x)\n",
    "        features = outputs.last_hidden_state\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "print(\"‚úì CvTClassifier defined (IMPROVED with higher dropout)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping basado en F1-Score (MEJORADO).\"\"\"\n",
    "\n",
    "    def __init__(self, patience=25, min_delta=1e-4, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "        elif self.mode == 'min':\n",
    "            if score < self.best_score - self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.best_epoch = epoch\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:  # mode == 'max'\n",
    "            if score > self.best_score + self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.best_epoch = epoch\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "\n",
    "print(\"‚úì EarlyStopping defined (monitoring F1-Score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive metrics.\"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'specificity': specificity,\n",
    "        'confusion_matrix': cm,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp\n",
    "    }\n",
    "\n",
    "print(\"‚úì calculate_metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler, use_mixup=False, \n",
    "                accumulation_steps=1, swa_model=None, use_swa=False):\n",
    "    \"\"\"Train one epoch with Mixup and Gradient Accumulation.\"\"\"\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Mixup\n",
    "        if use_mixup and np.random.rand() < CONFIG['mixup_prob']:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, CONFIG['mixup_alpha'])\n",
    "\n",
    "            with autocast(enabled=CONFIG['use_amp']):\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            with autocast(enabled=CONFIG['use_amp']):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "        # Normalize loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip_norm'])\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * accumulation_steps * images.size(0)\n",
    "\n",
    "        if not use_mixup or np.random.rand() >= CONFIG['mixup_prob']:\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item() * accumulation_steps:.4f}\",\n",
    "            'acc': f\"{100. * correct / total if total > 0 else 0:.2f}%\"\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total if total > 0 else 0\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì train_epoch function defined (with Mixup and Gradient Accumulation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def validate_epoch(model, loader, criterion, device, use_tta=False):\n",
    "    \"\"\"Validate one epoch with optional TTA.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validation\")\n",
    "\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if use_tta and CONFIG['use_tta']:\n",
    "                # Test-Time Augmentation\n",
    "                outputs_list = []\n",
    "                \n",
    "                # Original\n",
    "                outputs = model(images)\n",
    "                outputs_list.append(outputs)\n",
    "\n",
    "                # Augmented versions\n",
    "                for _ in range(CONFIG['tta_augmentations'] - 1):\n",
    "                    # Random horizontal flip\n",
    "                    if np.random.rand() > 0.5:\n",
    "                        aug_images = torch.flip(images, dims=[3])\n",
    "                    else:\n",
    "                        aug_images = images\n",
    "                    \n",
    "                    outputs_aug = model(aug_images)\n",
    "                    outputs_list.append(outputs_aug)\n",
    "\n",
    "                # Average predictions\n",
    "                outputs = torch.stack(outputs_list).mean(dim=0)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_preds, all_targets\n",
    "\n",
    "print(\"‚úì validate_epoch function defined (with TTA support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_with_cross_validation():\n",
    "    \"\"\"Train model with cross-validation and advanced techniques.\"\"\"\n",
    "\n",
    "    # Patient-based stratified k-fold\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    patient_labels = np.array([labels[patient_ids == p][0] for p in unique_patients])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "    fold_metrics = []\n",
    "    fold_histories = []\n",
    "    fold_cms = []\n",
    "\n",
    "    for fold, (train_patient_idx, val_patient_idx) in enumerate(skf.split(unique_patients, patient_labels)):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Get train/val patient IDs\n",
    "        train_patients = unique_patients[train_patient_idx]\n",
    "        val_patients = unique_patients[val_patient_idx]\n",
    "\n",
    "        # Get train/val indices\n",
    "        train_idx = np.isin(patient_ids, train_patients)\n",
    "        val_idx = np.isin(patient_ids, val_patients)\n",
    "\n",
    "        train_images = image_paths[train_idx]\n",
    "        train_labels = labels[train_idx]\n",
    "        val_images = image_paths[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "\n",
    "        print(f\"\\nüìä Fold {fold + 1} split:\")\n",
    "        print(f\"  Train: {len(train_images)} images from {len(train_patients)} patients\")\n",
    "        print(f\"  Val: {len(val_images)} images from {len(val_patients)} patients\")\n",
    "        print(f\"  Train - Benign: {np.sum(train_labels == 0)}, Malignant: {np.sum(train_labels == 1)}\")\n",
    "        print(f\"  Val - Benign: {np.sum(val_labels == 0)}, Malignant: {np.sum(val_labels == 1)}\")\n",
    "\n",
    "        # Create datasets\n",
    "        train_transform = get_transforms(train=True)\n",
    "        val_transform = get_transforms(train=False)\n",
    "\n",
    "        train_dataset = MammographyDataset(train_images, train_labels, train_transform)\n",
    "        val_dataset = MammographyDataset(val_images, val_labels, val_transform)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory']\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory']\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = CvTClassifier(\n",
    "            model_name=CONFIG['pretrained_model'],\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            pretrained=CONFIG['pretrained']\n",
    "        ).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        if CONFIG['use_focal_loss']:\n",
    "            criterion = FocalLoss(\n",
    "                alpha=CONFIG['focal_alpha'],\n",
    "                gamma=CONFIG['focal_gamma'],\n",
    "                label_smoothing=CONFIG['label_smoothing']\n",
    "            )\n",
    "            print(f\"\\nüéØ Using Focal Loss (alpha={CONFIG['focal_alpha']}, gamma={CONFIG['focal_gamma']})\")\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(\n",
    "                weight=class_weights if CONFIG['use_class_weights'] else None,\n",
    "                label_smoothing=CONFIG['label_smoothing']\n",
    "            )\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=CONFIG['lr_max'],\n",
    "            weight_decay=CONFIG['weight_decay'],\n",
    "            betas=CONFIG['betas']\n",
    "        )\n",
    "\n",
    "        # Warmup scheduler\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=CONFIG['warmup_start_factor'],\n",
    "            end_factor=1.0,\n",
    "            total_iters=CONFIG['warmup_epochs']\n",
    "        )\n",
    "\n",
    "        # Main scheduler\n",
    "        main_scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=CONFIG['cosine_T0'],\n",
    "            T_mult=CONFIG['cosine_T_mult'],\n",
    "            eta_min=CONFIG['cosine_eta_min']\n",
    "        )\n",
    "\n",
    "        # SWA\n",
    "        if CONFIG['use_swa']:\n",
    "            swa_model = AveragedModel(model)\n",
    "            swa_scheduler = SWALR(optimizer, swa_lr=CONFIG['swa_lr'])\n",
    "            print(f\"üìä SWA enabled (start epoch: {CONFIG['swa_start_epoch']})\")\n",
    "        else:\n",
    "            swa_model = None\n",
    "            swa_scheduler = None\n",
    "\n",
    "        # Mixed precision scaler\n",
    "        scaler = GradScaler(enabled=CONFIG['use_amp'])\n",
    "\n",
    "        # Early stopping (based on F1-Score)\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=CONFIG['early_stopping_patience'],\n",
    "            min_delta=CONFIG['min_delta'],\n",
    "            mode='max'  # Maximize F1-Score\n",
    "        )\n",
    "\n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'val_f1': [],\n",
    "            'lr': []\n",
    "        }\n",
    "\n",
    "        best_f1_score = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f\"\\nüöÄ Starting training for fold {fold + 1}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            print(f\"\\n{'‚îÄ'*80}\")\n",
    "            print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "            print(f\"{'‚îÄ'*80}\")\n",
    "\n",
    "            # Determine if using SWA this epoch\n",
    "            use_swa_this_epoch = CONFIG['use_swa'] and epoch >= CONFIG['swa_start_epoch']\n",
    "\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, scaler,\n",
    "                use_mixup=CONFIG['use_mixup'],\n",
    "                accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "                swa_model=swa_model if use_swa_this_epoch else None,\n",
    "                use_swa=use_swa_this_epoch\n",
    "            )\n",
    "\n",
    "            # Validate\n",
    "            val_loss, val_acc, val_preds, val_targets = validate_epoch(\n",
    "                model, val_loader, criterion, device, use_tta=False\n",
    "            )\n",
    "\n",
    "            # Calculate F1-Score for early stopping\n",
    "            val_f1 = f1_score(val_targets, val_preds, average='binary')\n",
    "\n",
    "            # Update SWA\n",
    "            if use_swa_this_epoch:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()\n",
    "\n",
    "            # Update scheduler\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            if epoch < CONFIG['warmup_epochs']:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                if CONFIG['use_swa'] and epoch >= CONFIG['swa_start_epoch']:\n",
    "                    # SWA scheduler already stepped\n",
    "                    pass\n",
    "                else:\n",
    "                    main_scheduler.step()\n",
    "\n",
    "            # Store history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['val_f1'].append(val_f1)\n",
    "            history['lr'].append(current_lr)\n",
    "\n",
    "            # Print epoch summary\n",
    "            phase = \"Warmup\" if epoch < CONFIG['warmup_epochs'] else (\"SWA\" if use_swa_this_epoch else \"Main\")\n",
    "            print(f\"\\nüìä Epoch {epoch + 1} Summary ({phase}):\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "            print(f\"  LR: {current_lr:.2e}\")\n",
    "\n",
    "            # Save best model (based on F1-Score)\n",
    "            if val_f1 > best_f1_score:\n",
    "                best_f1_score = val_f1\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "                model_path = os.path.join(\n",
    "                    CONFIG['output_dir'],\n",
    "                    f\"{CONFIG['model_name']}_fold{fold}.pth\"\n",
    "                )\n",
    "                \n",
    "                # Save SWA model if available, otherwise regular model\n",
    "                model_to_save = swa_model if use_swa_this_epoch else model\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model_to_save.state_dict() if use_swa_this_epoch else model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1,\n",
    "                    'config': CONFIG\n",
    "                }, model_path)\n",
    "\n",
    "                print(f\"  ‚úì Model saved: {model_path} (F1: {val_f1:.4f})\")\n",
    "\n",
    "            # Early stopping (only after warmup)\n",
    "            if epoch >= CONFIG['warmup_epochs']:\n",
    "                early_stopping(val_f1, epoch + 1)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    print(f\"  Best epoch: {best_epoch} with F1-Score: {best_f1_score:.4f}\")\n",
    "                    break\n",
    "\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\n‚è±Ô∏è Training time for fold {fold + 1}: {training_time/60:.2f} minutes\")\n",
    "\n",
    "        # Load best model for evaluation\n",
    "        model_path = os.path.join(CONFIG['output_dir'], f\"{CONFIG['model_name']}_fold{fold}.pth\")\n",
    "        checkpoint = torch.load(model_path)\n",
    "        \n",
    "        # Load to SWA model if used\n",
    "        if CONFIG['use_swa']:\n",
    "            # Update BN statistics\n",
    "            torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
    "            swa_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            eval_model = swa_model\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            eval_model = model\n",
    "\n",
    "        # Final evaluation with TTA\n",
    "        print(f\"\\nüìä Final evaluation on validation set (with TTA)...\")\n",
    "        _, _, final_preds, final_targets = validate_epoch(\n",
    "            eval_model, val_loader, criterion, device, use_tta=True\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(final_targets, final_preds)\n",
    "\n",
    "        print(f\"\\n‚úÖ Fold {fold + 1} Results:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  Precision: {metrics['precision']*100:.2f}%\")\n",
    "        print(f\"  Recall: {metrics['recall']*100:.2f}%\")\n",
    "        print(f\"  F1-Score: {metrics['f1']*100:.2f}%\")\n",
    "        print(f\"  Specificity: {metrics['specificity']*100:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        fold_metrics.append(metrics)\n",
    "        fold_histories.append(history)\n",
    "        fold_cms.append(metrics['confusion_matrix'])\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history, fold)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(metrics['confusion_matrix'], fold)\n",
    "\n",
    "        # Clean up\n",
    "        del model, optimizer, train_loader, val_loader\n",
    "        if CONFIG['use_swa']:\n",
    "            del swa_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return fold_metrics, fold_histories, fold_cms\n",
    "\n",
    "print(\"‚úì train_with_cross_validation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def plot_training_history(history, fold):\n",
    "    \"\"\"Plot training history with warmup and SWA indicators.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', markersize=3)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', markersize=3)\n",
    "    axes[0, 0].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    if CONFIG['use_swa']:\n",
    "        axes[0, 0].axvline(x=CONFIG['swa_start_epoch'], color='g', linestyle='--', alpha=0.5, label='SWA Start')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title(f'Fold {fold + 1}: Loss (CvT_TL_2)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0, 1].plot([acc*100 for acc in history['train_acc']], label='Train Acc', marker='o', markersize=3)\n",
    "    axes[0, 1].plot([acc*100 for acc in history['val_acc']], label='Val Acc', marker='s', markersize=3)\n",
    "    axes[0, 1].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    if CONFIG['use_swa']:\n",
    "        axes[0, 1].axvline(x=CONFIG['swa_start_epoch'], color='g', linestyle='--', alpha=0.5, label='SWA Start')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].set_title(f'Fold {fold + 1}: Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # F1-Score\n",
    "    axes[1, 0].plot([f1*100 for f1 in history['val_f1']], label='Val F1-Score', marker='d', markersize=3, color='purple')\n",
    "    axes[1, 0].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    if CONFIG['use_swa']:\n",
    "        axes[1, 0].axvline(x=CONFIG['swa_start_epoch'], color='g', linestyle='--', alpha=0.5, label='SWA Start')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1-Score (%)')\n",
    "    axes[1, 0].set_title(f'Fold {fold + 1}: F1-Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['lr'], marker='o', markersize=3)\n",
    "    axes[1, 1].axvline(x=CONFIG['warmup_epochs'], color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    if CONFIG['use_swa']:\n",
    "        axes[1, 1].axvline(x=CONFIG['swa_start_epoch'], color='g', linestyle='--', alpha=0.5, label='SWA Start')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title(f'Fold {fold + 1}: Learning Rate (Cosine Annealing)')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_fold{fold}_history.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"  History plot saved: {plot_path}\")\n",
    "\n",
    "def plot_confusion_matrix(cm, fold, normalize=False):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='.2f' if normalize else 'd',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(f'Fold {fold + 1}: Confusion Matrix')\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_fold{fold}_cm.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"  Confusion matrix saved: {plot_path}\")\n",
    "\n",
    "def save_metrics_summary(fold_metrics):\n",
    "    \"\"\"Save metrics summary to CSV.\"\"\"\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Fold': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Specificity': []\n",
    "    }\n",
    "\n",
    "    for fold, metrics in enumerate(fold_metrics):\n",
    "        metrics_dict['Fold'].append(fold + 1)\n",
    "        metrics_dict['Accuracy'].append(metrics['accuracy'])\n",
    "        metrics_dict['Precision'].append(metrics['precision'])\n",
    "        metrics_dict['Recall'].append(metrics['recall'])\n",
    "        metrics_dict['F1-Score'].append(metrics['f1'])\n",
    "        metrics_dict['Specificity'].append(metrics['specificity'])\n",
    "\n",
    "    # Calculate mean and std\n",
    "    for key in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']:\n",
    "        values = metrics_dict[key]\n",
    "        metrics_dict['Fold'].append('Mean ¬± Std')\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        metrics_dict[key].append(f\"{mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "        break\n",
    "\n",
    "    for key in ['Precision', 'Recall', 'F1-Score', 'Specificity']:\n",
    "        values = [m for m in metrics_dict[key] if isinstance(m, float)]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        metrics_dict[key].append(f\"{mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "    df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "    csv_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    )\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Metrics saved to: {csv_path}\")\n",
    "    print(f\"\\n{df.to_string(index=False)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_average_confusion_matrix(fold_cms):\n",
    "    \"\"\"Plot average confusion matrix across all folds.\"\"\"\n",
    "\n",
    "    avg_cm = np.mean(fold_cms, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        avg_cm,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Average Confusion Matrix (5-Fold CV - CvT_TL_2)')\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_avg_cm.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n‚úÖ Average confusion matrix saved: {plot_path}\")\n",
    "\n",
    "def plot_mean_confusion_matrix(fold_cms):\n",
    "    \"\"\"\n",
    "    NUEVO: Plot mean confusion matrix with both absolute values and percentages.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_cm = np.mean(fold_cms, axis=0)\n",
    "    \n",
    "    # Normalize by rows (percentages)\n",
    "    mean_cm_normalized = mean_cm / mean_cm.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Absolute values\n",
    "    sns.heatmap(\n",
    "        mean_cm,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Mean Count'},\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_title('Mean Confusion Matrix - Absolute Values')\n",
    "    \n",
    "    # Percentages\n",
    "    sns.heatmap(\n",
    "        mean_cm_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2%',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benign', 'Malignant'],\n",
    "        yticklabels=['Benign', 'Malignant'],\n",
    "        cbar_kws={'label': 'Percentage'},\n",
    "        ax=axes[1],\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    axes[1].set_ylabel('True Label')\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    axes[1].set_title('Mean Confusion Matrix - Normalized by Row')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(\n",
    "        CONFIG['metrics_dir'],\n",
    "        f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    )\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Mean confusion matrix saved: {plot_path}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nüìä Mean Confusion Matrix Statistics:\")\n",
    "    print(f\"  True Negatives (TN): {mean_cm[0, 0]:.1f}\")\n",
    "    print(f\"  False Positives (FP): {mean_cm[0, 1]:.1f}\")\n",
    "    print(f\"  False Negatives (FN): {mean_cm[1, 0]:.1f}\")\n",
    "    print(f\"  True Positives (TP): {mean_cm[1, 1]:.1f}\")\n",
    "    print(f\"\\n  Sensitivity (Recall): {mean_cm_normalized[1, 1]:.2%}\")\n",
    "    print(f\"  Specificity: {mean_cm_normalized[0, 0]:.2%}\")\n",
    "\n",
    "print(\"‚úì Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING 5-FOLD CROSS VALIDATION TRAINING (CvT_TL_2 - ADVANCED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüöÄ New Features:\")\n",
    "print(\"  ‚úì Focal Loss\")\n",
    "print(\"  ‚úì Class Weights\")\n",
    "print(\"  ‚úì Mixup Augmentation\")\n",
    "print(\"  ‚úì Cosine Annealing Scheduler\")\n",
    "print(\"  ‚úì Stochastic Weight Averaging (SWA)\")\n",
    "print(\"  ‚úì Test-Time Augmentation (TTA)\")\n",
    "print(\"  ‚úì Gradient Accumulation\")\n",
    "print(\"  ‚úì Improved Augmentation (NO SCALE)\")\n",
    "print(\"  ‚úì Higher Dropout (0.5)\")\n",
    "print(\"  ‚úì F1-Score based Early Stopping\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "fold_metrics, fold_histories, fold_cms = train_with_cross_validation()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚è±Ô∏è Total training time: {total_time/3600:.2f} hours\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df = save_metrics_summary(fold_metrics)\n",
    "\n",
    "# Plot average confusion matrix\n",
    "plot_average_confusion_matrix(fold_cms)\n",
    "\n",
    "# NUEVO: Plot mean confusion matrix\n",
    "plot_mean_confusion_matrix(fold_cms)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DONE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ Results saved to:\")\n",
    "print(f\"  Models: {CONFIG['output_dir']}\")\n",
    "print(f\"  Metrics: {CONFIG['metrics_dir']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
