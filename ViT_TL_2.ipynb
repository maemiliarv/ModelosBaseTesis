{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# ViT_TL_2 - VERSIÓN MEJORADA CON OPTIMIZACIONES AVANZADAS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    # Optimizaciones de memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # Configurar asignación de memoria expandible\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "    print(\"\\n✓ Optimizaciones de memoria aplicadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 2. CONFIGURACIÓN DE PATHS Y PARÁMETROS MEJORADOS\n",
    "# ============================================================================\n",
    "\n",
    "# Paths base\n",
    "BASE_DIR = Path(\"/home/merivadeneira\")\n",
    "MASAS_DIR = BASE_DIR / \"Masas\"\n",
    "OUTPUT_DIR = BASE_DIR / \"Outputs\" / \"ViT\"\n",
    "METRICS_DIR = BASE_DIR / \"Metrics\" / \"ViT\"\n",
    "\n",
    "# Crear directorios\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuración del modelo MEJORADA\n",
    "CONFIG = {\n",
    "    # Datos\n",
    "    'input_channels': 3,\n",
    "    'input_size': 224,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Transfer Learning\n",
    "    'pretrained_model': 'google/vit-base-patch16-224',\n",
    "    'freeze_backbone': True,\n",
    "    'unfreeze_after_epoch': 5,  # CAMBIADO: De 10 a 5\n",
    "    'gradual_unfreeze': True,  # NUEVO: Descongelar gradualmente\n",
    "    'unfreeze_layers_per_epoch': 2,  # NUEVO\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 16,  # CAMBIADO: De 32 a 16\n",
    "    'gradient_accumulation_steps': 4,  # NUEVO: Batch efectivo = 64\n",
    "    'num_epochs': 100,\n",
    "    'k_folds': 5,\n",
    "    'early_stopping_patience': 35,  # CAMBIADO: De 25 a 35\n",
    "    'early_stopping_min_delta': 0.001,  # NUEVO\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'base_lr': 5e-5,  # CAMBIADO: De 1e-4 a 5e-5\n",
    "    'backbone_lr': 5e-6,  # CAMBIADO: De 1e-5 a 5e-6\n",
    "    'weight_decay': 0.05,  # CAMBIADO: De 0.01 a 0.05\n",
    "    'warmup_epochs': 5,  # NUEVO\n",
    "    'min_lr': 1e-7,  # NUEVO\n",
    "\n",
    "    # Scheduler\n",
    "    'lr_scheduler': 'CosineAnnealingWarmRestarts',  # CAMBIADO\n",
    "    'scheduler_T_0': 10,  # NUEVO\n",
    "    'scheduler_T_mult': 2,  # NUEVO\n",
    "    'scheduler_eta_min': 1e-7,  # NUEVO\n",
    "\n",
    "    # Regularization\n",
    "    'dropout_rate': 0.3,  # NUEVO\n",
    "    'label_smoothing': 0.1,  # NUEVO\n",
    "    'mixup_alpha': 0.2,  # NUEVO\n",
    "\n",
    "    # Loss Function\n",
    "    'use_focal_loss': True,  # NUEVO\n",
    "    'focal_loss_alpha': 0.25,  # NUEVO\n",
    "    'focal_loss_gamma': 2.0,  # NUEVO\n",
    "    'use_class_weights': True,  # NUEVO\n",
    "\n",
    "    # Augmentation (MÁS AGRESIVO)\n",
    "    'rotation_degrees': 20,  # CAMBIADO: De 15 a 20\n",
    "    'translate': 0.15,  # CAMBIADO: De 0.1 a 0.15\n",
    "    'horizontal_flip_prob': 0.5,  # NUEVO\n",
    "    'vertical_flip_prob': 0.3,  # NUEVO\n",
    "    'gaussian_blur_prob': 0.3,  # NUEVO\n",
    "    'gaussian_blur_kernel': 5,\n",
    "    'gaussian_blur_sigma': (0.1, 1.0),  # CAMBIADO: De (0.1, 0.5) a (0.1, 1.0)\n",
    "\n",
    "    # Test-Time Augmentation\n",
    "    'use_tta': True,  # NUEVO\n",
    "    'tta_transforms': 5,  # NUEVO\n",
    "\n",
    "    # Normalization\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "\n",
    "    # Model name\n",
    "    'model_name': 'ViT_TL_2'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACIÓN MEJORADA - ViT_TL_2\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:35s}: {value}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 3. UTILIDADES PARA PREVENIR DATA LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "def extract_patient_id(filename, database):\n",
    "    \"\"\"Extrae el patient ID del nombre del archivo\"\"\"\n",
    "    if database == 'DDSM':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"DDSM_{parts[1]}\"\n",
    "\n",
    "    elif database == 'INbreast':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 1:\n",
    "            return f\"INbreast_{parts[0]}\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "def load_image_paths_with_patient_ids():\n",
    "    \"\"\"Carga todas las rutas de imágenes con sus patient IDs y labels\"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    # Procesar DDSM\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        ddsm_path = MASAS_DIR / \"DDSM\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if ddsm_path.exists():\n",
    "            for img_file in ddsm_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'DDSM')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'DDSM'\n",
    "                })\n",
    "\n",
    "    # Procesar INbreast\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        inbreast_path = MASAS_DIR / \"INbreast\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if inbreast_path.exists():\n",
    "            for img_file in inbreast_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'INbreast')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'INbreast'\n",
    "                })\n",
    "\n",
    "    data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    print(f\"\\nTotal de imágenes cargadas: {len(data_df)}\")\n",
    "    print(f\"  - DDSM: {len(data_df[data_df['database']=='DDSM'])}\")\n",
    "    print(f\"  - INbreast: {len(data_df[data_df['database']=='INbreast'])}\")\n",
    "    print(f\"\\nDistribución de clases:\")\n",
    "    print(f\"  - Benignas (0): {len(data_df[data_df['label']==0])}\")\n",
    "    print(f\"  - Malignas (1): {len(data_df[data_df['label']==1])}\")\n",
    "    print(f\"\\nTotal de pacientes únicos: {data_df['patient_id'].nunique()}\")\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def create_patient_level_splits(data_df, k_folds=5, random_state=42):\n",
    "    \"\"\"Crea splits de K-Fold a nivel de paciente\"\"\"\n",
    "    patient_labels = data_df.groupby('patient_id')['label'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "\n",
    "    patient_labels.columns = ['patient_id', 'label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_splits = []\n",
    "    for fold_idx, (train_patient_idx, val_patient_idx) in enumerate(\n",
    "        skf.split(patient_labels['patient_id'], patient_labels['label'])\n",
    "    ):\n",
    "        train_patients = patient_labels.iloc[train_patient_idx]['patient_id'].values\n",
    "        val_patients = patient_labels.iloc[val_patient_idx]['patient_id'].values\n",
    "\n",
    "        train_indices = data_df[data_df['patient_id'].isin(train_patients)].index.tolist()\n",
    "        val_indices = data_df[data_df['patient_id'].isin(val_patients)].index.tolist()\n",
    "\n",
    "        fold_splits.append((train_indices, val_indices))\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(f\"  Train: {len(train_indices)} images from {len(train_patients)} patients\")\n",
    "        print(f\"  Val:   {len(val_indices)} images from {len(val_patients)} patients\")\n",
    "\n",
    "    return fold_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 4. DATASET CON AUGMENTACIONES MEJORADAS\n",
    "# ============================================================================\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Dataset con augmentaciones mejoradas\"\"\"\n",
    "\n",
    "    def __init__(self, data_df, indices, transform=None):\n",
    "        self.data = data_df.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.loc[idx, 'image_path']\n",
    "        label = self.data.loc[idx, 'label']\n",
    "\n",
    "        # Cargar imagen\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_transforms(config, is_training=True):\n",
    "    \"\"\"Transformaciones mejoradas con más augmentations\"\"\"\n",
    "\n",
    "    if is_training:\n",
    "        transform_list = [\n",
    "            transforms.Resize((config['input_size'], config['input_size'])),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=config['rotation_degrees'],\n",
    "                translate=(config['translate'], config['translate'])\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=config['horizontal_flip_prob']),\n",
    "            transforms.RandomVerticalFlip(p=config['vertical_flip_prob']),\n",
    "        ]\n",
    "\n",
    "        # Gaussian Blur con probabilidad\n",
    "        if config['gaussian_blur_prob'] > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(\n",
    "                        kernel_size=config['gaussian_blur_kernel'],\n",
    "                        sigma=config['gaussian_blur_sigma']\n",
    "                    )\n",
    "                ], p=config['gaussian_blur_prob'])\n",
    "            )\n",
    "\n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config['mean'], std=config['std'])\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        transform_list = [\n",
    "            transforms.Resize((config['input_size'], config['input_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config['mean'], std=config['std'])\n",
    "        ]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# 5. FOCAL LOSS Y LABEL SMOOTHING\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss para manejar desbalance de clases\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross Entropy con Label Smoothing\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = F.log_softmax(pred, dim=-1)\n",
    "\n",
    "        loss = -log_preds.sum(dim=-1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, reduction='mean')\n",
    "\n",
    "        return self.smoothing * loss / n_classes + (1 - self.smoothing) * nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 6. MIXUP DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"MixUp augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Loss para MixUp\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 7. MODELO ViT CON DROPOUT\n",
    "# ============================================================================\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    \"\"\"ViT con dropout para regularización\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(config['pretrained_model'])\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        # Clasificador con dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(config['dropout_rate']),\n",
    "            nn.Linear(hidden_size, config['num_classes'])\n",
    "        )\n",
    "\n",
    "        # Congelar backbone si se especifica\n",
    "        if config['freeze_backbone']:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Descongelar todo el backbone\"\"\"\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def gradual_unfreeze(self, num_layers=2):\n",
    "        \"\"\"Descongelar gradualmente capas del encoder\"\"\"\n",
    "        encoder_layers = list(self.vit.encoder.layer)\n",
    "\n",
    "        # Descongelar las últimas num_layers capas\n",
    "        for layer in encoder_layers[-num_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 8. WARMUP LEARNING RATE SCHEDULER\n",
    "# ============================================================================\n",
    "\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Warmup scheduler para aumentar LR gradualmente\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_epochs, base_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 9. TRAINING CON TODAS LAS MEJORAS\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, config, epoch, warmup_scheduler=None):\n",
    "    \"\"\"Entrenamiento de una época con todas las mejoras\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Warmup en las primeras épocas\n",
    "    if warmup_scheduler and epoch < config['warmup_epochs']:\n",
    "        warmup_scheduler.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # MixUp augmentation\n",
    "        if config['mixup_alpha'] > 0 and np.random.random() > 0.5:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, config['mixup_alpha'])\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Gradient accumulation\n",
    "        loss = loss / config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Métricas\n",
    "        running_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (batch_idx + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device, use_tta=False, tta_transforms=5):\n",
    "    \"\"\"Validación con opción de Test-Time Augmentation\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            if use_tta:\n",
    "                # Test-Time Augmentation\n",
    "                tta_outputs = []\n",
    "                for _ in range(tta_transforms):\n",
    "                    # Aplicar transformaciones aleatorias\n",
    "                    augmented = images.clone()\n",
    "                    if torch.rand(1) > 0.5:\n",
    "                        augmented = torch.flip(augmented, dims=[3])  # Horizontal flip\n",
    "                    if torch.rand(1) > 0.7:\n",
    "                        augmented = torch.flip(augmented, dims=[2])  # Vertical flip\n",
    "\n",
    "                    outputs = model(augmented)\n",
    "                    tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "\n",
    "                # Promediar predicciones\n",
    "                outputs = torch.stack(tta_outputs).mean(dim=0)\n",
    "                loss = criterion(torch.log(outputs + 1e-8), labels)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "\n",
    "    # Calcular métricas\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy * 100,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 10. MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INICIANDO ENTRENAMIENTO - ViT_TL_2\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    print(\"[1/5] Cargando datos...\")\n",
    "    data_df = load_image_paths_with_patient_ids()\n",
    "    fold_splits = create_patient_level_splits(data_df, k_folds=CONFIG['k_folds'])\n",
    "\n",
    "    # 2. Calcular class weights\n",
    "    if CONFIG['use_class_weights']:\n",
    "        class_counts = data_df['label'].value_counts().sort_index()\n",
    "        total_samples = len(data_df)\n",
    "        class_weights = torch.FloatTensor([\n",
    "            total_samples / (len(class_counts) * class_counts[i])\n",
    "            for i in range(CONFIG['num_classes'])\n",
    "        ]).to(device)\n",
    "        print(f\"\\nClass weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    # Almacenar resultados\n",
    "    all_fold_results = []\n",
    "    all_confusion_matrices = []\n",
    "    metrics_summary = defaultdict(list)\n",
    "    epoch_metrics_all_folds = []  # NUEVO: Para guardar métricas por época\n",
    "\n",
    "    # 3. K-Fold Cross Validation\n",
    "    print(f\"\\n[2/5] Iniciando {CONFIG['k_folds']}-Fold Cross Validation...\")\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FOLD {fold_idx + 1}/{CONFIG['k_folds']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Datasets\n",
    "        train_transform = get_transforms(CONFIG, is_training=True)\n",
    "        val_transform = get_transforms(CONFIG, is_training=False)\n",
    "\n",
    "        train_dataset = MammographyDataset(data_df, train_indices, train_transform)\n",
    "        val_dataset = MammographyDataset(data_df, val_indices, val_transform)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Modelo\n",
    "        model = ViTClassifier(CONFIG).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        if CONFIG['use_focal_loss']:\n",
    "            criterion = FocalLoss(\n",
    "                alpha=CONFIG['focal_loss_alpha'],\n",
    "                gamma=CONFIG['focal_loss_gamma']\n",
    "            )\n",
    "        elif CONFIG['label_smoothing'] > 0:\n",
    "            criterion = LabelSmoothingCrossEntropy(smoothing=CONFIG['label_smoothing'])\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # Optimizer con diferentes LR para backbone y clasificador\n",
    "        optimizer = AdamW([\n",
    "            {'params': model.classifier.parameters(), 'lr': CONFIG['base_lr']},\n",
    "            {'params': model.vit.parameters(), 'lr': CONFIG['backbone_lr']}\n",
    "        ], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=CONFIG['scheduler_T_0'],\n",
    "            T_mult=CONFIG['scheduler_T_mult'],\n",
    "            eta_min=CONFIG['scheduler_eta_min']\n",
    "        )\n",
    "\n",
    "        # Warmup scheduler\n",
    "        warmup_scheduler = WarmupScheduler(\n",
    "            optimizer,\n",
    "            CONFIG['warmup_epochs'],\n",
    "            CONFIG['base_lr']\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "            'val_precision': [], 'val_recall': [], 'val_specificity': []\n",
    "        }\n",
    "\n",
    "        fold_epoch_metrics = []  # NUEVO: Métricas de este fold por época\n",
    "\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            # Descongelar backbone gradualmente\n",
    "            if epoch == CONFIG['unfreeze_after_epoch']:\n",
    "                if CONFIG['gradual_unfreeze']:\n",
    "                    print(f\"\\n🔓 Descongelando {CONFIG['unfreeze_layers_per_epoch']} capas del backbone...\")\n",
    "                    model.gradual_unfreeze(CONFIG['unfreeze_layers_per_epoch'])\n",
    "                else:\n",
    "                    print(f\"\\n🔓 Descongelando todo el backbone...\")\n",
    "                    model.unfreeze_backbone()\n",
    "\n",
    "            # Train\n",
    "            train_loss, train_acc, train_f1 = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, CONFIG, epoch, warmup_scheduler\n",
    "            )\n",
    "\n",
    "            # Validate\n",
    "            val_metrics = validate_epoch(\n",
    "                model, val_loader, criterion, device,\n",
    "                use_tta=CONFIG['use_tta'],\n",
    "                tta_transforms=CONFIG['tta_transforms']\n",
    "            )\n",
    "\n",
    "            # Scheduler step\n",
    "            if epoch >= CONFIG['warmup_epochs']:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Guardar historia\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['train_f1'].append(train_f1)\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_acc'].append(val_metrics['accuracy'])\n",
    "            history['val_f1'].append(val_metrics['f1'])\n",
    "            history['val_precision'].append(val_metrics['precision'])\n",
    "            history['val_recall'].append(val_metrics['recall'])\n",
    "            history['val_specificity'].append(val_metrics['specificity'])\n",
    "\n",
    "            # NUEVO: Guardar métricas por época\n",
    "            fold_epoch_metrics.append({\n",
    "                'fold': fold_idx + 1,\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'train_f1': train_f1,\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_acc': val_metrics['accuracy'],\n",
    "                'val_f1': val_metrics['f1'],\n",
    "                'val_precision': val_metrics['precision'],\n",
    "                'val_recall': val_metrics['recall'],\n",
    "                'val_specificity': val_metrics['specificity']\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "            print(f\"  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.4f}\")\n",
    "            print(f\"  Val   - Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.2f}% | F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"  Val   - Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f} | Spec: {val_metrics['specificity']:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_metrics['f1'] > best_val_f1 + CONFIG['early_stopping_min_delta']:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Guardar mejor modelo\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_f1': best_val_f1,\n",
    "                    'config': CONFIG,\n",
    "                    'history': history\n",
    "                }, OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_best.pth\")\n",
    "\n",
    "                print(f\"  ✓ Mejor modelo guardado (F1: {best_val_f1:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"  Patience: {patience_counter}/{CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "            if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Guardar métricas de época de este fold\n",
    "        epoch_metrics_all_folds.extend(fold_epoch_metrics)\n",
    "\n",
    "        # Cargar mejor modelo\n",
    "        checkpoint = torch.load(\n",
    "          OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_best.pth\",\n",
    "          map_location=device,\n",
    "          weights_only=False\n",
    "      )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Validación final\n",
    "        final_val_metrics = validate_epoch(\n",
    "            model, val_loader, criterion, device,\n",
    "            use_tta=CONFIG['use_tta'],\n",
    "            tta_transforms=CONFIG['tta_transforms']\n",
    "        )\n",
    "\n",
    "        # Guardar resultados del fold\n",
    "        fold_results = {\n",
    "            'fold': fold_idx + 1,\n",
    "            'best_epoch': checkpoint['epoch'] + 1,\n",
    "            'f1': final_val_metrics['f1'],\n",
    "            'precision': final_val_metrics['precision'],\n",
    "            'recall': final_val_metrics['recall'],\n",
    "            'specificity': final_val_metrics['specificity'],\n",
    "            'accuracy': final_val_metrics['accuracy'],\n",
    "            'confusion_matrix': final_val_metrics['confusion_matrix']\n",
    "        }\n",
    "\n",
    "        all_fold_results.append(fold_results)\n",
    "        all_confusion_matrices.append(final_val_metrics['confusion_matrix'])\n",
    "\n",
    "        for key in ['f1', 'precision', 'recall', 'specificity', 'accuracy']:\n",
    "            metrics_summary[key].append(fold_results[key])\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1} completado:\")\n",
    "        print(f\"  F1: {fold_results['f1']:.4f} | Precision: {fold_results['precision']:.4f} | Recall: {fold_results['recall']:.4f}\")\n",
    "        print(f\"  Specificity: {fold_results['specificity']:.4f} | Accuracy: {fold_results['accuracy']:.2f}%\")\n",
    "\n",
    "        # Matriz de confusión individual\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm_percent = final_val_metrics['confusion_matrix'].astype('float') / final_val_metrics['confusion_matrix'].sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "        sns.heatmap(\n",
    "            final_val_metrics['confusion_matrix'],\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Benigna', 'Maligna'],\n",
    "            yticklabels=['Benigna', 'Maligna'],\n",
    "            cbar_kws={'label': 'Count'}\n",
    "        )\n",
    "\n",
    "        # Añadir porcentajes\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                plt.text(j + 0.5, i + 0.7, f'({cm_percent[i, j]:.1f}%)',\n",
    "                        ha='center', va='center', color='red', fontsize=9)\n",
    "\n",
    "        plt.title(f'Confusion Matrix - Fold {fold_idx + 1}', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_confusion_matrix.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Guardar métricas finales\n",
    "    print(\"\\n[4/5] Guardando métricas finales...\")\n",
    "\n",
    "    # ARCHIVO 1: ViT_TL_2_metrics.csv (métricas por fold con promedio)\n",
    "    metrics_data = []\n",
    "    for result in all_fold_results:\n",
    "        metrics_data.append({\n",
    "            'Fold': result['fold'],\n",
    "            'F1': result['f1'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'Specificity': result['specificity'],\n",
    "            'Accuracy': result['accuracy']\n",
    "        })\n",
    "\n",
    "    # Añadir fila de promedios\n",
    "    metrics_data.append({\n",
    "        'Fold': 'Mean ± Std',\n",
    "        'F1': f\"{np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\",\n",
    "        'Precision': f\"{np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\",\n",
    "        'Recall': f\"{np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\",\n",
    "        'Specificity': f\"{np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\",\n",
    "        'Accuracy': f\"{np.mean(metrics_summary['accuracy']):.2f} ± {np.std(metrics_summary['accuracy']):.2f}\"\n",
    "    })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    print(f\"✓ Métricas guardadas: {metrics_csv_path}\")\n",
    "\n",
    "    # ARCHIVO 2: ViT_TL_2_metrics_summary.csv (métricas por época para todos los folds)\n",
    "    summary_df = pd.DataFrame(epoch_metrics_all_folds)\n",
    "    summary_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics_summary.csv\"\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"✓ Resumen de métricas guardado: {summary_csv_path}\")\n",
    "\n",
    "    # ARCHIVO 3: ViT_TL_2_mean_confusion_matrix.png\n",
    "    print(\"\\n[5/5] Creando matriz de confusión promedio...\")\n",
    "    mean_cm = np.mean(all_confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        mean_cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.title(f'Average Confusion Matrix - {CONFIG[\"model_name\"]}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Añadir métricas\n",
    "    metrics_text = f\"\"\"Mean Metrics:\n",
    "F1-Score: {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\n",
    "Precision: {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\n",
    "Recall: {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\n",
    "Specificity: {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\n",
    "Accuracy: {np.mean(metrics_summary['accuracy']):.2f} ± {np.std(metrics_summary['accuracy']):.2f}\"\"\"\n",
    "\n",
    "    plt.text(\n",
    "        2.5, 0.5, metrics_text,\n",
    "        fontsize=10,\n",
    "        verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "    mean_cm_path = METRICS_DIR / f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(mean_cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Matriz de confusión promedio guardada: {mean_cm_path}\")\n",
    "\n",
    "    # Guardar configuración\n",
    "    config_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=4)\n",
    "    print(f\"✓ Configuración guardada: {config_path}\")\n",
    "\n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ ENTRENAMIENTO COMPLETADO - ViT_TL_2\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nRESULTADOS FINALES ({CONFIG['k_folds']}-Fold Cross Validation):\")\n",
    "    print(f\"  F1-Score:     {np.mean(metrics_summary['f1']):.4f} ± {np.std(metrics_summary['f1']):.4f}\")\n",
    "    print(f\"  Precision:    {np.mean(metrics_summary['precision']):.4f} ± {np.std(metrics_summary['precision']):.4f}\")\n",
    "    print(f\"  Recall:       {np.mean(metrics_summary['recall']):.4f} ± {np.std(metrics_summary['recall']):.4f}\")\n",
    "    print(f\"  Specificity:  {np.mean(metrics_summary['specificity']):.4f} ± {np.std(metrics_summary['specificity']):.4f}\")\n",
    "    print(f\"  Accuracy:     {np.mean(metrics_summary['accuracy']):.2f}% ± {np.std(metrics_summary['accuracy']):.2f}%\")\n",
    "    print(\"\\nARCHIVOS GENERADOS:\")\n",
    "    print(f\"  ✓ {CONFIG['model_name']}_metrics.csv\")\n",
    "    print(f\"  ✓ {CONFIG['model_name']}_metrics_summary.csv\")\n",
    "    print(f\"  ✓ {CONFIG['model_name']}_mean_confusion_matrix.png\")\n",
    "    print(f\"  ✓ {CONFIG['k_folds']} modelos (.pth)\")\n",
    "    print(f\"  ✓ {CONFIG['k_folds']} matrices de confusión individuales\")\n",
    "    print(f\"  ✓ 1 archivo de configuración (.json)\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 11. EJECUTAR\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
