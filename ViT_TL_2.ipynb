{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# ViT_TL_2 - VERSIÃ“N MEJORADA CON OPTIMIZACIONES AVANZADAS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers (Hugging Face)\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    # Optimizaciones de memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # Configurar asignaciÃ³n de memoria expandible\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "    print(\"\\nâœ“ Optimizaciones de memoria aplicadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 2. CONFIGURACIÃ“N DE PATHS Y PARÃMETROS MEJORADOS\n",
    "# ============================================================================\n",
    "\n",
    "# Paths base\n",
    "BASE_DIR = Path(\"/home/merivadeneira\")\n",
    "MASAS_DIR = BASE_DIR / \"Masas\"\n",
    "OUTPUT_DIR = BASE_DIR / \"Outputs\" / \"ViT\"\n",
    "METRICS_DIR = BASE_DIR / \"Metrics\" / \"ViT\"\n",
    "\n",
    "# Crear directorios\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ConfiguraciÃ³n del modelo MEJORADA\n",
    "CONFIG = {\n",
    "    # Datos\n",
    "    'input_channels': 3,\n",
    "    'input_size': 224,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Transfer Learning\n",
    "    'pretrained_model': 'google/vit-base-patch16-224',\n",
    "    'freeze_backbone': True,\n",
    "    'unfreeze_after_epoch': 5,  # CAMBIADO: De 10 a 5\n",
    "    'gradual_unfreeze': True,  # NUEVO: Descongelar gradualmente\n",
    "    'unfreeze_layers_per_epoch': 2,  # NUEVO\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 16,  # CAMBIADO: De 32 a 16\n",
    "    'gradient_accumulation_steps': 4,  # NUEVO: Batch efectivo = 64\n",
    "    'num_epochs': 100,\n",
    "    'k_folds': 5,\n",
    "    'early_stopping_patience': 35,  # CAMBIADO: De 25 a 35\n",
    "    'early_stopping_min_delta': 0.001,  # NUEVO\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'base_lr': 5e-5,  # CAMBIADO: De 1e-4 a 5e-5\n",
    "    'backbone_lr': 5e-6,  # CAMBIADO: De 1e-5 a 5e-6\n",
    "    'weight_decay': 0.05,  # CAMBIADO: De 0.01 a 0.05\n",
    "    'warmup_epochs': 5,  # NUEVO\n",
    "    'min_lr': 1e-7,  # NUEVO\n",
    "\n",
    "    # Scheduler\n",
    "    'lr_scheduler': 'CosineAnnealingWarmRestarts',  # CAMBIADO\n",
    "    'scheduler_T_0': 10,  # NUEVO\n",
    "    'scheduler_T_mult': 2,  # NUEVO\n",
    "    'scheduler_eta_min': 1e-7,  # NUEVO\n",
    "\n",
    "    # Regularization\n",
    "    'dropout_rate': 0.3,  # NUEVO\n",
    "    'label_smoothing': 0.1,  # NUEVO\n",
    "    'mixup_alpha': 0.2,  # NUEVO\n",
    "\n",
    "    # Loss Function\n",
    "    'use_focal_loss': True,  # NUEVO\n",
    "    'focal_loss_alpha': 0.25,  # NUEVO\n",
    "    'focal_loss_gamma': 2.0,  # NUEVO\n",
    "    'use_class_weights': True,  # NUEVO\n",
    "\n",
    "    # Augmentation (MÃS AGRESIVO)\n",
    "    'rotation_degrees': 20,  # CAMBIADO: De 15 a 20\n",
    "    'translate': 0.15,  # CAMBIADO: De 0.1 a 0.15\n",
    "    'horizontal_flip_prob': 0.5,  # NUEVO\n",
    "    'vertical_flip_prob': 0.3,  # NUEVO\n",
    "    'gaussian_blur_prob': 0.3,  # NUEVO\n",
    "    'gaussian_blur_kernel': 5,\n",
    "    'gaussian_blur_sigma': (0.1, 1.0),  # CAMBIADO: De (0.1, 0.5) a (0.1, 1.0)\n",
    "\n",
    "    # Test-Time Augmentation\n",
    "    'use_tta': True,  # NUEVO\n",
    "    'tta_transforms': 5,  # NUEVO\n",
    "\n",
    "    # Normalization\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "\n",
    "    # Model name\n",
    "    'model_name': 'ViT_TL_2'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACIÃ“N MEJORADA - ViT_TL_2\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:35s}: {value}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 3. UTILIDADES PARA PREVENIR DATA LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "def extract_patient_id(filename, database):\n",
    "    \"\"\"Extrae el patient ID del nombre del archivo\"\"\"\n",
    "    if database == 'DDSM':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"DDSM_{parts[1]}\"\n",
    "\n",
    "    elif database == 'INbreast':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 1:\n",
    "            return f\"INbreast_{parts[0]}\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "def load_image_paths_with_patient_ids():\n",
    "    \"\"\"Carga todas las rutas de imÃ¡genes con sus patient IDs y labels\"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    # Procesar DDSM\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        ddsm_path = MASAS_DIR / \"DDSM\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if ddsm_path.exists():\n",
    "            for img_file in ddsm_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'DDSM')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'DDSM'\n",
    "                })\n",
    "\n",
    "    # Procesar INbreast\n",
    "    for label_name, label_value in [('Benignas', 0), ('Malignas', 1)]:\n",
    "        inbreast_path = MASAS_DIR / \"INbreast\" / label_name / \"Resized_512\"\n",
    "\n",
    "        if inbreast_path.exists():\n",
    "            for img_file in inbreast_path.glob(\"*.png\"):\n",
    "                patient_id = extract_patient_id(img_file.name, 'INbreast')\n",
    "                data_list.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': label_value,\n",
    "                    'database': 'INbreast'\n",
    "                })\n",
    "\n",
    "    data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    print(f\"\\nTotal de imÃ¡genes cargadas: {len(data_df)}\")\n",
    "    print(f\"  - DDSM: {len(data_df[data_df['database']=='DDSM'])}\")\n",
    "    print(f\"  - INbreast: {len(data_df[data_df['database']=='INbreast'])}\")\n",
    "    print(f\"\\nDistribuciÃ³n de clases:\")\n",
    "    print(f\"  - Benignas (0): {len(data_df[data_df['label']==0])}\")\n",
    "    print(f\"  - Malignas (1): {len(data_df[data_df['label']==1])}\")\n",
    "    print(f\"\\nTotal de pacientes Ãºnicos: {data_df['patient_id'].nunique()}\")\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def create_patient_level_splits(data_df, k_folds=5, random_state=42):\n",
    "    \"\"\"Crea splits de K-Fold a nivel de paciente\"\"\"\n",
    "    patient_labels = data_df.groupby('patient_id')['label'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "\n",
    "    patient_labels.columns = ['patient_id', 'label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_splits = []\n",
    "    for fold_idx, (train_patient_idx, val_patient_idx) in enumerate(\n",
    "        skf.split(patient_labels['patient_id'], patient_labels['label'])\n",
    "    ):\n",
    "        train_patients = patient_labels.iloc[train_patient_idx]['patient_id'].values\n",
    "        val_patients = patient_labels.iloc[val_patient_idx]['patient_id'].values\n",
    "\n",
    "        train_indices = data_df[data_df['patient_id'].isin(train_patients)].index.tolist()\n",
    "        val_indices = data_df[data_df['patient_id'].isin(val_patients)].index.tolist()\n",
    "\n",
    "        fold_splits.append((train_indices, val_indices))\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(f\"  Train: {len(train_indices)} images from {len(train_patients)} patients\")\n",
    "        print(f\"  Val:   {len(val_indices)} images from {len(val_patients)} patients\")\n",
    "\n",
    "    return fold_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 4. DATASET CON AUGMENTACIONES MEJORADAS\n",
    "# ============================================================================\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    \"\"\"Dataset con augmentaciones mejoradas\"\"\"\n",
    "\n",
    "    def __init__(self, data_df, indices, transform=None):\n",
    "        self.data = data_df.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.loc[idx, 'image_path']\n",
    "        label = self.data.loc[idx, 'label']\n",
    "\n",
    "        # Cargar imagen\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_transforms(config, is_training=True):\n",
    "    \"\"\"Transformaciones mejoradas con mÃ¡s augmentations\"\"\"\n",
    "\n",
    "    if is_training:\n",
    "        transform_list = [\n",
    "            transforms.Resize((config['input_size'], config['input_size'])),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=config['rotation_degrees'],\n",
    "                translate=(config['translate'], config['translate'])\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=config['horizontal_flip_prob']),\n",
    "            transforms.RandomVerticalFlip(p=config['vertical_flip_prob']),\n",
    "        ]\n",
    "\n",
    "        # Gaussian Blur con probabilidad\n",
    "        if config['gaussian_blur_prob'] > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(\n",
    "                        kernel_size=config['gaussian_blur_kernel'],\n",
    "                        sigma=config['gaussian_blur_sigma']\n",
    "                    )\n",
    "                ], p=config['gaussian_blur_prob'])\n",
    "            )\n",
    "\n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config['mean'], std=config['std'])\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        transform_list = [\n",
    "            transforms.Resize((config['input_size'], config['input_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config['mean'], std=config['std'])\n",
    "        ]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# 5. FOCAL LOSS Y LABEL SMOOTHING\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss para manejar desbalance de clases\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross Entropy con Label Smoothing\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = F.log_softmax(pred, dim=-1)\n",
    "\n",
    "        loss = -log_preds.sum(dim=-1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, reduction='mean')\n",
    "\n",
    "        return self.smoothing * loss / n_classes + (1 - self.smoothing) * nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 6. MIXUP DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"MixUp augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Loss para MixUp\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 7. MODELO ViT CON DROPOUT\n",
    "# ============================================================================\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    \"\"\"ViT con dropout para regularizaciÃ³n\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "\n",
    "        self.vit = ViTModel.from_pretrained(config['pretrained_model'])\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        # Clasificador con dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(config['dropout_rate']),\n",
    "            nn.Linear(hidden_size, config['num_classes'])\n",
    "        )\n",
    "\n",
    "        # Congelar backbone si se especifica\n",
    "        if config['freeze_backbone']:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Descongelar todo el backbone\"\"\"\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def gradual_unfreeze(self, num_layers=2):\n",
    "        \"\"\"Descongelar gradualmente capas del encoder\"\"\"\n",
    "        encoder_layers = list(self.vit.encoder.layer)\n",
    "\n",
    "        # Descongelar las Ãºltimas num_layers capas\n",
    "        for layer in encoder_layers[-num_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 8. WARMUP LEARNING RATE SCHEDULER\n",
    "# ============================================================================\n",
    "\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Warmup scheduler para aumentar LR gradualmente\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_epochs, base_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 9. TRAINING CON TODAS LAS MEJORAS\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, config, epoch, warmup_scheduler=None):\n",
    "    \"\"\"Entrenamiento de una Ã©poca con todas las mejoras\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Warmup en las primeras Ã©pocas\n",
    "    if warmup_scheduler and epoch < config['warmup_epochs']:\n",
    "        warmup_scheduler.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # MixUp augmentation\n",
    "        if config['mixup_alpha'] > 0 and np.random.random() > 0.5:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, config['mixup_alpha'])\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Gradient accumulation\n",
    "        loss = loss / config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # MÃ©tricas\n",
    "        running_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (batch_idx + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device, use_tta=False, tta_transforms=5):\n",
    "    \"\"\"ValidaciÃ³n con opciÃ³n de Test-Time Augmentation\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            if use_tta:\n",
    "                # Test-Time Augmentation\n",
    "                tta_outputs = []\n",
    "                for _ in range(tta_transforms):\n",
    "                    # Aplicar transformaciones aleatorias\n",
    "                    augmented = images.clone()\n",
    "                    if torch.rand(1) > 0.5:\n",
    "                        augmented = torch.flip(augmented, dims=[3])  # Horizontal flip\n",
    "                    if torch.rand(1) > 0.7:\n",
    "                        augmented = torch.flip(augmented, dims=[2])  # Vertical flip\n",
    "\n",
    "                    outputs = model(augmented)\n",
    "                    tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "\n",
    "                # Promediar predicciones\n",
    "                outputs = torch.stack(tta_outputs).mean(dim=0)\n",
    "                loss = criterion(torch.log(outputs + 1e-8), labels)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "\n",
    "    # Calcular mÃ©tricas\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy * 100,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 10. MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INICIANDO ENTRENAMIENTO - ViT_TL_2\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # 1. Cargar datos\n",
    "    print(\"[1/5] Cargando datos...\")\n",
    "    data_df = load_image_paths_with_patient_ids()\n",
    "    fold_splits = create_patient_level_splits(data_df, k_folds=CONFIG['k_folds'])\n",
    "\n",
    "    # 2. Calcular class weights\n",
    "    if CONFIG['use_class_weights']:\n",
    "        class_counts = data_df['label'].value_counts().sort_index()\n",
    "        total_samples = len(data_df)\n",
    "        class_weights = torch.FloatTensor([\n",
    "            total_samples / (len(class_counts) * class_counts[i])\n",
    "            for i in range(CONFIG['num_classes'])\n",
    "        ]).to(device)\n",
    "        print(f\"\\nClass weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    # Almacenar resultados\n",
    "    all_fold_results = []\n",
    "    all_confusion_matrices = []\n",
    "    metrics_summary = defaultdict(list)\n",
    "    epoch_metrics_all_folds = []  # NUEVO: Para guardar mÃ©tricas por Ã©poca\n",
    "\n",
    "    # 3. K-Fold Cross Validation\n",
    "    print(f\"\\n[2/5] Iniciando {CONFIG['k_folds']}-Fold Cross Validation...\")\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FOLD {fold_idx + 1}/{CONFIG['k_folds']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Datasets\n",
    "        train_transform = get_transforms(CONFIG, is_training=True)\n",
    "        val_transform = get_transforms(CONFIG, is_training=False)\n",
    "\n",
    "        train_dataset = MammographyDataset(data_df, train_indices, train_transform)\n",
    "        val_dataset = MammographyDataset(data_df, val_indices, val_transform)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Modelo\n",
    "        model = ViTClassifier(CONFIG).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        if CONFIG['use_focal_loss']:\n",
    "            criterion = FocalLoss(\n",
    "                alpha=CONFIG['focal_loss_alpha'],\n",
    "                gamma=CONFIG['focal_loss_gamma']\n",
    "            )\n",
    "        elif CONFIG['label_smoothing'] > 0:\n",
    "            criterion = LabelSmoothingCrossEntropy(smoothing=CONFIG['label_smoothing'])\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # Optimizer con diferentes LR para backbone y clasificador\n",
    "        optimizer = AdamW([\n",
    "            {'params': model.classifier.parameters(), 'lr': CONFIG['base_lr']},\n",
    "            {'params': model.vit.parameters(), 'lr': CONFIG['backbone_lr']}\n",
    "        ], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=CONFIG['scheduler_T_0'],\n",
    "            T_mult=CONFIG['scheduler_T_mult'],\n",
    "            eta_min=CONFIG['scheduler_eta_min']\n",
    "        )\n",
    "\n",
    "        # Warmup scheduler\n",
    "        warmup_scheduler = WarmupScheduler(\n",
    "            optimizer,\n",
    "            CONFIG['warmup_epochs'],\n",
    "            CONFIG['base_lr']\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "            'val_precision': [], 'val_recall': [], 'val_specificity': []\n",
    "        }\n",
    "\n",
    "        fold_epoch_metrics = []  # NUEVO: MÃ©tricas de este fold por Ã©poca\n",
    "\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            # Descongelar backbone gradualmente\n",
    "            if epoch == CONFIG['unfreeze_after_epoch']:\n",
    "                if CONFIG['gradual_unfreeze']:\n",
    "                    print(f\"\\nðŸ”“ Descongelando {CONFIG['unfreeze_layers_per_epoch']} capas del backbone...\")\n",
    "                    model.gradual_unfreeze(CONFIG['unfreeze_layers_per_epoch'])\n",
    "                else:\n",
    "                    print(f\"\\nðŸ”“ Descongelando todo el backbone...\")\n",
    "                    model.unfreeze_backbone()\n",
    "\n",
    "            # Train\n",
    "            train_loss, train_acc, train_f1 = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, CONFIG, epoch, warmup_scheduler\n",
    "            )\n",
    "\n",
    "            # Validate\n",
    "            val_metrics = validate_epoch(\n",
    "                model, val_loader, criterion, device,\n",
    "                use_tta=CONFIG['use_tta'],\n",
    "                tta_transforms=CONFIG['tta_transforms']\n",
    "            )\n",
    "\n",
    "            # Scheduler step\n",
    "            if epoch >= CONFIG['warmup_epochs']:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Guardar historia\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['train_f1'].append(train_f1)\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_acc'].append(val_metrics['accuracy'])\n",
    "            history['val_f1'].append(val_metrics['f1'])\n",
    "            history['val_precision'].append(val_metrics['precision'])\n",
    "            history['val_recall'].append(val_metrics['recall'])\n",
    "            history['val_specificity'].append(val_metrics['specificity'])\n",
    "\n",
    "            # NUEVO: Guardar mÃ©tricas por Ã©poca\n",
    "            fold_epoch_metrics.append({\n",
    "                'fold': fold_idx + 1,\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'train_f1': train_f1,\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_acc': val_metrics['accuracy'],\n",
    "                'val_f1': val_metrics['f1'],\n",
    "                'val_precision': val_metrics['precision'],\n",
    "                'val_recall': val_metrics['recall'],\n",
    "                'val_specificity': val_metrics['specificity']\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "            print(f\"  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.4f}\")\n",
    "            print(f\"  Val   - Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.2f}% | F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"  Val   - Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f} | Spec: {val_metrics['specificity']:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_metrics['f1'] > best_val_f1 + CONFIG['early_stopping_min_delta']:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Guardar mejor modelo\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_f1': best_val_f1,\n",
    "                    'config': CONFIG,\n",
    "                    'history': history\n",
    "                }, OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_best.pth\")\n",
    "\n",
    "                print(f\"  âœ“ Mejor modelo guardado (F1: {best_val_f1:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"  Patience: {patience_counter}/{CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "            if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "                print(f\"\\nâš  Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Guardar mÃ©tricas de Ã©poca de este fold\n",
    "        epoch_metrics_all_folds.extend(fold_epoch_metrics)\n",
    "\n",
    "        # Cargar mejor modelo\n",
    "        checkpoint = torch.load(\n",
    "          OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_best.pth\",\n",
    "          map_location=device,\n",
    "          weights_only=False\n",
    "      )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # ValidaciÃ³n final\n",
    "        final_val_metrics = validate_epoch(\n",
    "            model, val_loader, criterion, device,\n",
    "            use_tta=CONFIG['use_tta'],\n",
    "            tta_transforms=CONFIG['tta_transforms']\n",
    "        )\n",
    "\n",
    "        # Guardar resultados del fold\n",
    "        fold_results = {\n",
    "            'fold': fold_idx + 1,\n",
    "            'best_epoch': checkpoint['epoch'] + 1,\n",
    "            'f1': final_val_metrics['f1'],\n",
    "            'precision': final_val_metrics['precision'],\n",
    "            'recall': final_val_metrics['recall'],\n",
    "            'specificity': final_val_metrics['specificity'],\n",
    "            'accuracy': final_val_metrics['accuracy'],\n",
    "            'confusion_matrix': final_val_metrics['confusion_matrix']\n",
    "        }\n",
    "\n",
    "        all_fold_results.append(fold_results)\n",
    "        all_confusion_matrices.append(final_val_metrics['confusion_matrix'])\n",
    "\n",
    "        for key in ['f1', 'precision', 'recall', 'specificity', 'accuracy']:\n",
    "            metrics_summary[key].append(fold_results[key])\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1} completado:\")\n",
    "        print(f\"  F1: {fold_results['f1']:.4f} | Precision: {fold_results['precision']:.4f} | Recall: {fold_results['recall']:.4f}\")\n",
    "        print(f\"  Specificity: {fold_results['specificity']:.4f} | Accuracy: {fold_results['accuracy']:.2f}%\")\n",
    "\n",
    "        # Matriz de confusiÃ³n individual\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm_percent = final_val_metrics['confusion_matrix'].astype('float') / final_val_metrics['confusion_matrix'].sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "        sns.heatmap(\n",
    "            final_val_metrics['confusion_matrix'],\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Benigna', 'Maligna'],\n",
    "            yticklabels=['Benigna', 'Maligna'],\n",
    "            cbar_kws={'label': 'Count'}\n",
    "        )\n",
    "\n",
    "        # AÃ±adir porcentajes\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                plt.text(j + 0.5, i + 0.7, f'({cm_percent[i, j]:.1f}%)',\n",
    "                        ha='center', va='center', color='red', fontsize=9)\n",
    "\n",
    "        plt.title(f'Confusion Matrix - Fold {fold_idx + 1}', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(METRICS_DIR / f\"{CONFIG['model_name']}_fold{fold_idx+1}_confusion_matrix.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Guardar mÃ©tricas finales\n",
    "    print(\"\\n[4/5] Guardando mÃ©tricas finales...\")\n",
    "\n",
    "    # ARCHIVO 1: ViT_TL_2_metrics.csv (mÃ©tricas por fold con promedio)\n",
    "    metrics_data = []\n",
    "    for result in all_fold_results:\n",
    "        metrics_data.append({\n",
    "            'Fold': result['fold'],\n",
    "            'F1': result['f1'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'Specificity': result['specificity'],\n",
    "            'Accuracy': result['accuracy']\n",
    "        })\n",
    "\n",
    "    # AÃ±adir fila de promedios\n",
    "    metrics_data.append({\n",
    "        'Fold': 'Mean Â± Std',\n",
    "        'F1': f\"{np.mean(metrics_summary['f1']):.4f} Â± {np.std(metrics_summary['f1']):.4f}\",\n",
    "        'Precision': f\"{np.mean(metrics_summary['precision']):.4f} Â± {np.std(metrics_summary['precision']):.4f}\",\n",
    "        'Recall': f\"{np.mean(metrics_summary['recall']):.4f} Â± {np.std(metrics_summary['recall']):.4f}\",\n",
    "        'Specificity': f\"{np.mean(metrics_summary['specificity']):.4f} Â± {np.std(metrics_summary['specificity']):.4f}\",\n",
    "        'Accuracy': f\"{np.mean(metrics_summary['accuracy']):.2f} Â± {np.std(metrics_summary['accuracy']):.2f}\"\n",
    "    })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    print(f\"âœ“ MÃ©tricas guardadas: {metrics_csv_path}\")\n",
    "\n",
    "    # ARCHIVO 2: ViT_TL_2_metrics_summary.csv (mÃ©tricas por Ã©poca para todos los folds)\n",
    "    summary_df = pd.DataFrame(epoch_metrics_all_folds)\n",
    "    summary_csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics_summary.csv\"\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"âœ“ Resumen de mÃ©tricas guardado: {summary_csv_path}\")\n",
    "\n",
    "    # ARCHIVO 3: ViT_TL_2_mean_confusion_matrix.png\n",
    "    print(\"\\n[5/5] Creando matriz de confusiÃ³n promedio...\")\n",
    "    mean_cm = np.mean(all_confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        mean_cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Benigna', 'Maligna'],\n",
    "        yticklabels=['Benigna', 'Maligna'],\n",
    "        cbar_kws={'label': 'Average Count'}\n",
    "    )\n",
    "    plt.title(f'Average Confusion Matrix - {CONFIG[\"model_name\"]}',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # AÃ±adir mÃ©tricas\n",
    "    metrics_text = f\"\"\"Mean Metrics:\n",
    "F1-Score: {np.mean(metrics_summary['f1']):.4f} Â± {np.std(metrics_summary['f1']):.4f}\n",
    "Precision: {np.mean(metrics_summary['precision']):.4f} Â± {np.std(metrics_summary['precision']):.4f}\n",
    "Recall: {np.mean(metrics_summary['recall']):.4f} Â± {np.std(metrics_summary['recall']):.4f}\n",
    "Specificity: {np.mean(metrics_summary['specificity']):.4f} Â± {np.std(metrics_summary['specificity']):.4f}\n",
    "Accuracy: {np.mean(metrics_summary['accuracy']):.2f} Â± {np.std(metrics_summary['accuracy']):.2f}\"\"\"\n",
    "\n",
    "    plt.text(\n",
    "        2.5, 0.5, metrics_text,\n",
    "        fontsize=10,\n",
    "        verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "\n",
    "    mean_cm_path = METRICS_DIR / f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(mean_cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Matriz de confusiÃ³n promedio guardada: {mean_cm_path}\")\n",
    "\n",
    "    # Guardar configuraciÃ³n\n",
    "    config_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=4)\n",
    "    print(f\"âœ“ ConfiguraciÃ³n guardada: {config_path}\")\n",
    "\n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ“ ENTRENAMIENTO COMPLETADO - ViT_TL_2\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nRESULTADOS FINALES ({CONFIG['k_folds']}-Fold Cross Validation):\")\n",
    "    print(f\"  F1-Score:     {np.mean(metrics_summary['f1']):.4f} Â± {np.std(metrics_summary['f1']):.4f}\")\n",
    "    print(f\"  Precision:    {np.mean(metrics_summary['precision']):.4f} Â± {np.std(metrics_summary['precision']):.4f}\")\n",
    "    print(f\"  Recall:       {np.mean(metrics_summary['recall']):.4f} Â± {np.std(metrics_summary['recall']):.4f}\")\n",
    "    print(f\"  Specificity:  {np.mean(metrics_summary['specificity']):.4f} Â± {np.std(metrics_summary['specificity']):.4f}\")\n",
    "    print(f\"  Accuracy:     {np.mean(metrics_summary['accuracy']):.2f}% Â± {np.std(metrics_summary['accuracy']):.2f}%\")\n",
    "    print(\"\\nARCHIVOS GENERADOS:\")\n",
    "    print(f\"  âœ“ {CONFIG['model_name']}_metrics.csv\")\n",
    "    print(f\"  âœ“ {CONFIG['model_name']}_metrics_summary.csv\")\n",
    "    print(f\"  âœ“ {CONFIG['model_name']}_mean_confusion_matrix.png\")\n",
    "    print(f\"  âœ“ {CONFIG['k_folds']} modelos (.pth)\")\n",
    "    print(f\"  âœ“ {CONFIG['k_folds']} matrices de confusiÃ³n individuales\")\n",
    "    print(f\"  âœ“ 1 archivo de configuraciÃ³n (.json)\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 11. EJECUTAR\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
