{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "!pip install -q transformers datasets pillow timm\n",
    "\n",
    "print(\"✓ Dependencias instaladas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os, gc, json, time, warnings, math, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CvtModel, CvtConfig\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_curve, auc, precision_recall_curve)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# Configuración principal (ajusta rutas si hace falta)\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'ddsm_benign_path': '/home/merivadeneira/Masas/DDSM/Benignas/Resized_512',\n",
    "    'ddsm_malign_path': '/home/merivadeneira/Masas/DDSM/Malignas/Resized_512',\n",
    "    'inbreast_benign_path': '/home/merivadeneira/Masas/INbreast/Benignas/Resized_512',\n",
    "    'inbreast_malign_path': '/home/merivadeneira/Masas/INbreast/Malignas/Resized_512',\n",
    "    'output_dir': '/home/merivadeneira/Outputs/CvT',\n",
    "    'metrics_dir': '/home/merivadeneira/Metrics/CvT',\n",
    "\n",
    "    # Modelo\n",
    "    'model_name': 'CvT_TL_1_HF',\n",
    "    'pretrained_model': 'microsoft/cvt-13',\n",
    "    'input_size': 512,\n",
    "    'num_classes': 2,\n",
    "\n",
    "    # Entrenamiento\n",
    "    'batch_size': 16,            # sube a 32 si la GPU lo permite\n",
    "    'num_epochs': 100,\n",
    "    'num_folds': 5,\n",
    "    'early_stopping_patience': 12,\n",
    "    'min_delta': 1e-4,\n",
    "    'use_amp': True,\n",
    "\n",
    "    # LRs por grupos\n",
    "    'lr_head': 3e-4,\n",
    "    'lr_last': 1e-4,\n",
    "    'lr_rest': 3e-5,\n",
    "\n",
    "    # Warmup + unfreezing\n",
    "    'warmup_epochs': 3,\n",
    "\n",
    "    # Scheduler (cosine restarts)\n",
    "    'eta_min': 1e-6,\n",
    "    'T_0': 10,\n",
    "    'T_mult': 1,\n",
    "\n",
    "    # Optimizer\n",
    "    'weight_decay': 0.01,\n",
    "    'betas': (0.9, 0.999),\n",
    "\n",
    "    # Label smoothing y focal toggle\n",
    "    'label_smoothing': 0.1,\n",
    "    'use_focal': False,\n",
    "    'focal_gamma': 1.5,\n",
    "\n",
    "    # Aumentaciones\n",
    "    'hflip_p': 0.5,\n",
    "    'vflip_p': 0.0,          # desactivado\n",
    "    'rot_deg': 10,\n",
    "    'brightness': 0.1,\n",
    "    'contrast': 0.1,\n",
    "    'random_erasing_p': 0.1,\n",
    "    'mixup_p': 0.2,\n",
    "    'cutmix_p': 0.2,\n",
    "\n",
    "    # TTA\n",
    "    'tta_do': True,\n",
    "\n",
    "    # Reproducibilidad\n",
    "    'seed': 42,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['metrics_dir'], exist_ok=True)\n",
    "with open(os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_config.json\"), \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CONFIG['seed'])\n",
    "print(\"✓ Config listo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_patient_id(filename, dataset='ddsm'):\n",
    "    if dataset == 'ddsm':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0]}_{parts[1]}\"\n",
    "    elif dataset == 'inbreast':\n",
    "        return filename.split('_')[0]\n",
    "    return filename\n",
    "\n",
    "def load_dataset():\n",
    "    image_paths, labels, patient_ids = [], [], []\n",
    "    datasets = [\n",
    "        (CONFIG['ddsm_benign_path'], 0, 'ddsm'),\n",
    "        (CONFIG['ddsm_malign_path'], 1, 'ddsm'),\n",
    "        (CONFIG['inbreast_benign_path'], 0, 'inbreast'),\n",
    "        (CONFIG['inbreast_malign_path'], 1, 'inbreast'),\n",
    "    ]\n",
    "    for path, label, name in datasets:\n",
    "        if not os.path.exists(path):\n",
    "            print(\"WARNING path not found:\", path); continue\n",
    "        files = [f for f in os.listdir(path) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "        for fn in files:\n",
    "            image_paths.append(os.path.join(path, fn))\n",
    "            labels.append(label)\n",
    "            patient_ids.append(extract_patient_id(fn, name))\n",
    "    return image_paths, labels, patient_ids\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert('L').convert('RGB')  # gris→RGB\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]; std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=CONFIG['hflip_p']),\n",
    "            transforms.RandomRotation(CONFIG['rot_deg'], fill=0),\n",
    "            transforms.ColorJitter(brightness=CONFIG['brightness'], contrast=CONFIG['contrast']),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            transforms.RandomErasing(p=CONFIG['random_erasing_p']),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "image_paths, labels, patient_ids = load_dataset()\n",
    "print(f\"Total imágenes: {len(image_paths)} | Benign: {labels.count(0)} | Malignant: {labels.count(1)} | Pacientes: {len(set(patient_ids))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) o (B, L, C) -> convertimos a BCHW si hace falta\n",
    "        if x.dim() == 3:  # (B, L, C) -> (B, C, L, 1)\n",
    "            x = x.permute(0,2,1).unsqueeze(-1)\n",
    "        x = torch.clamp(x, min=self.eps)\n",
    "        x = x.pow(self.p).mean(dim=(-1,-2)).pow(1./self.p)  # (B,C)\n",
    "        return x\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=1.5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = (1-pt)**self.gamma * ce\n",
    "        return loss.mean() if self.reduction=='mean' else loss.sum()\n",
    "\n",
    "class ModelEmaV2:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.ema = type(model)()\n",
    "        self.ema.load_state_dict(model.state_dict())\n",
    "        self.ema.to(next(model.parameters()).device)\n",
    "        self.decay = decay\n",
    "        for p in self.ema.parameters(): p.requires_grad_(False)\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        msd = model.state_dict()\n",
    "        for k, v in self.ema.state_dict().items():\n",
    "            if v.dtype.is_floating_point:\n",
    "                v.copy_(v * self.decay + msd[k] * (1. - self.decay))\n",
    "            else:\n",
    "                v.copy_(msd[k])\n",
    "\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat); cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W); cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W); y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W); y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mixup_cutmix(inputs, targets):\n",
    "    B, C, H, W = inputs.shape\n",
    "    # Decide si aplicar algo\n",
    "    r = random.random()\n",
    "    if r < CONFIG['mixup_p']:\n",
    "        lam = np.random.beta(0.4, 0.4)\n",
    "        perm = torch.randperm(B, device=inputs.device)\n",
    "        mixed = lam * inputs + (1-lam) * inputs[perm]\n",
    "        return mixed, targets, targets[perm], lam, 'mixup'\n",
    "    elif r < CONFIG['mixup_p'] + CONFIG['cutmix_p']:\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        perm = torch.randperm(B, device=inputs.device)\n",
    "        x1,y1,x2,y2 = rand_bbox(W,H,lam)\n",
    "        mixed = inputs.clone()\n",
    "        mixed[:, :, y1:y2, x1:x2] = inputs[perm, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2-x1)*(y2-y1) / (W*H))\n",
    "        return mixed, targets, targets[perm], lam, 'cutmix'\n",
    "    else:\n",
    "        return inputs, targets, targets, 1.0, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CvTForImageClassification(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/cvt-13', num_classes=2, image_size=512, pretrained=True):\n",
    "        super().__init__()\n",
    "        cfg = CvtConfig.from_pretrained(model_name)\n",
    "        cfg.image_size = image_size\n",
    "        self.cvt = CvtModel.from_pretrained(model_name, config=cfg, ignore_mismatched_sizes=True) if pretrained else CvtModel(cfg)\n",
    "\n",
    "        hidden = self.cvt.config.embed_dim[-1]  # 384 en cvt-13\n",
    "        self.pool = GeM()\n",
    "        self.norm = nn.LayerNorm(hidden)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden//2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cvt(pixel_values=x, return_dict=True)\n",
    "        feats = out.last_hidden_state  # (B, L, C)\n",
    "        x = self.pool(feats)           # (B, C)\n",
    "        x = self.norm(x)\n",
    "        logits = self.mlp(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_param_groups(model):\n",
    "    head = []\n",
    "    last = []\n",
    "    rest = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        if 'mlp' in n or 'norm' in n and 'mlp' in n:  # head\n",
    "            head.append(p)\n",
    "        elif 'stages' in n and '.2.' in n:  # último stage\n",
    "            last.append(p)\n",
    "        else:\n",
    "            rest.append(p)\n",
    "    return [\n",
    "        {'params': rest, 'lr': CONFIG['lr_rest']},\n",
    "        {'params': last, 'lr': CONFIG['lr_last']},\n",
    "        {'params': head, 'lr': CONFIG['lr_head']},\n",
    "    ]\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    y = np.array(y)\n",
    "    n0 = (y==0).sum(); n1 = (y==1).sum()\n",
    "    n = len(y); w0 = n/(2*n0) if n0>0 else 1.0; w1 = n/(2*n1) if n1>0 else 1.0\n",
    "    return torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
    "\n",
    "def metrics_from_preds(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    spec = (cm[0,0]/(cm[0,0]+cm[0,1])) if (cm.shape==(2,2) and (cm[0,0]+cm[0,1])>0) else 0.0\n",
    "    return acc, prec, rec, f1, spec, cm\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_loader(model, loader, tta=False):\n",
    "    model.eval()\n",
    "    all_prob, all_t = [], []\n",
    "    for x, t in loader:\n",
    "        x = x.to(device); t = torch.as_tensor(t, device=device)\n",
    "        if tta and CONFIG['tta_do']:\n",
    "            # identidad\n",
    "            logits = model(x)\n",
    "            # hflip\n",
    "            logits += model(torch.flip(x, dims=[3]))\n",
    "            # rotate +5°\n",
    "            rot1 = transforms.functional.rotate(x, 5, fill=0)\n",
    "            logits += model(rot1)\n",
    "            # rotate -5°\n",
    "            rot2 = transforms.functional.rotate(x, -5, fill=0)\n",
    "            logits += model(rot2)\n",
    "            logits = logits / 4.0\n",
    "        else:\n",
    "            logits = model(x)\n",
    "        prob1 = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
    "        all_prob.append(prob1); all_t.append(t.cpu().numpy())\n",
    "    return np.concatenate(all_prob), np.concatenate(all_t)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, criterion, use_mix=True):\n",
    "    model.train()\n",
    "    running = 0.0; total=0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = torch.as_tensor(y, device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_mix:\n",
    "            x, y1, y2, lam, kind = apply_mixup_cutmix(x, y)\n",
    "        else:\n",
    "            y1, y2, lam, kind = y, y, 1.0, None\n",
    "\n",
    "        with autocast(enabled=CONFIG['use_amp']):\n",
    "            logits = model(x)\n",
    "            if kind is None:\n",
    "                loss = criterion(logits, y1)\n",
    "            else:\n",
    "                loss = lam*criterion(logits, y1) + (1-lam)*criterion(logits, y2)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        running += loss.item() * x.size(0); total += x.size(0)\n",
    "    return running/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=12, min_delta=1e-4):\n",
    "        self.patience = patience; self.min_delta = min_delta\n",
    "        self.best = None; self.count = 0; self.stop = False\n",
    "    def step(self, val):\n",
    "        if self.best is None or val < self.best - self.min_delta:\n",
    "            self.best = val; self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience: self.stop = True\n",
    "\n",
    "def plot_cm(cm, title, path):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Benign','Malignant'], yticklabels=['Benign','Malignant'])\n",
    "    plt.title(title); plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def train_cv():\n",
    "    # Agrupar por paciente\n",
    "    p2idx, p2lab = defaultdict(list), {}\n",
    "    for i,(pid,lab) in enumerate(zip(patient_ids, labels)):\n",
    "        p2idx[pid].append(i); p2lab[pid]=lab\n",
    "    up = list(p2idx.keys())\n",
    "    y_up = [p2lab[p] for p in up]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "    fold_metrics_val, fold_metrics_train, cms = [], [], []\n",
    "\n",
    "    # Para CSV historia por fold\n",
    "    history_rows = []\n",
    "\n",
    "    for fold,(tr_idx, va_idx) in enumerate(skf.split(up, y_up), start=1):\n",
    "        tr_p = [up[i] for i in tr_idx]; va_p = [up[i] for i in va_idx]\n",
    "        tr_ids = sum([p2idx[p] for p in tr_p], [])\n",
    "        va_ids = sum([p2idx[p] for p in va_p], [])\n",
    "\n",
    "        tr_paths = [image_paths[i] for i in tr_ids]\n",
    "        va_paths = [image_paths[i] for i in va_ids]\n",
    "        tr_y = [labels[i] for i in tr_ids]\n",
    "        va_y = [labels[i] for i in va_ids]\n",
    "\n",
    "        ds_tr = MammographyDataset(tr_paths, tr_y, get_transforms(True))\n",
    "        ds_va = MammographyDataset(va_paths, va_y, get_transforms(False))\n",
    "        ld_tr = DataLoader(ds_tr, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                           num_workers=CONFIG['num_workers'], pin_memory=CONFIG['pin_memory'])\n",
    "        ld_va = DataLoader(ds_va, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                           num_workers=CONFIG['num_workers'], pin_memory=CONFIG['pin_memory'])\n",
    "\n",
    "        model = CvTForImageClassification(CONFIG['pretrained_model'], CONFIG['num_classes'],\n",
    "                                          CONFIG['input_size'], pretrained=True).to(device)\n",
    "\n",
    "        # Congelar backbone en warmup\n",
    "        for n,p in model.named_parameters():\n",
    "            if 'mlp' in n: p.requires_grad_(True)\n",
    "            else: p.requires_grad_(False)\n",
    "\n",
    "        groups = build_param_groups(model)\n",
    "        optimizer = AdamW(groups, weight_decay=CONFIG['weight_decay'], betas=CONFIG['betas'])\n",
    "        scaler = GradScaler(enabled=CONFIG['use_amp'])\n",
    "        # scheduler con cosine restarts\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'], eta_min=CONFIG['eta_min'])\n",
    "\n",
    "        # Loss con weights del fold\n",
    "        cls_weights = compute_class_weights(tr_y)\n",
    "        if CONFIG['use_focal']:\n",
    "            criterion = FocalLoss(weight=cls_weights, gamma=CONFIG['focal_gamma'])\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(weight=cls_weights, label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "        ema = ModelEmaV2(model, decay=0.999)\n",
    "        early = EarlyStopping(CONFIG['early_stopping_patience'], CONFIG['min_delta'])\n",
    "\n",
    "        best_val = float('inf'); best_path = os.path.join(CONFIG['output_dir'], f\"{CONFIG['model_name']}_fold{fold}.pth\")\n",
    "\n",
    "        for epoch in range(1, CONFIG['num_epochs']+1):\n",
    "            # Warmup fases: primeras epochs solo head; luego unfreeze parcial\n",
    "            if epoch == CONFIG['warmup_epochs']+1:\n",
    "                for n,p in model.named_parameters():\n",
    "                    if 'stages' in n and '.2.' in n: p.requires_grad_(True)   # último stage\n",
    "                    elif 'mlp' in n: p.requires_grad_(True)\n",
    "                    else: p.requires_grad_(True)  # también liberamos resto (si prefieres gradual, cambia esto)\n",
    "\n",
    "                groups = build_param_groups(model)\n",
    "                optimizer = AdamW(groups, weight_decay=CONFIG['weight_decay'], betas=CONFIG['betas'])\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'], eta_min=CONFIG['eta_min'])\n",
    "\n",
    "            tr_loss = train_one_epoch(model, ld_tr, optimizer, scaler, criterion, use_mix=True)\n",
    "            ema.update(model)\n",
    "            scheduler.step(epoch + fold/10.0)  # paso suave\n",
    "\n",
    "            # Eval con EMA (más estable)\n",
    "            probs_val, y_val = predict_loader(ema.ema, ld_va, tta=True)\n",
    "            # Busco umbral que maximiza F1\n",
    "            best_thr, best_f1, best_pack = 0.5, -1, None\n",
    "            for thr in np.linspace(0.2, 0.8, 61):\n",
    "                acc,prec,rec,f1,spec,cm = metrics_from_preds(y_val, probs_val, thr)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1, best_thr, best_pack = f1, thr, (acc,prec,rec,f1,spec,cm)\n",
    "            acc,prec,rec,f1,spec,cm = best_pack\n",
    "            val_loss_proxy = 1 - f1  # proxy simple para early stopping\n",
    "\n",
    "            history_rows.append({\n",
    "                'fold': fold, 'epoch': epoch, 'train_loss': tr_loss,\n",
    "                'val_f1': f1, 'val_acc': acc, 'val_prec': prec, 'val_rec': rec, 'val_spec': spec,\n",
    "                'best_thr': best_thr\n",
    "            })\n",
    "\n",
    "            if val_loss_proxy < best_val:\n",
    "                best_val = val_loss_proxy\n",
    "                torch.save({'model': ema.ema.state_dict(),\n",
    "                            'thr': best_thr,\n",
    "                            #'config': CONFIG}, best_path)\n",
    "                },\n",
    "                 best_path\n",
    "                )\n",
    "\n",
    "            early.step(val_loss_proxy)\n",
    "            print(f\"[Fold {fold} | Epoch {epoch}] tr_loss={tr_loss:.4f} | val_f1={f1:.4f} @thr={best_thr:.2f}\")\n",
    "            if early.stop:\n",
    "                print(f\"Early stopping en fold {fold} (epoch {epoch})\")\n",
    "                break\n",
    "\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "        # Cargar mejor modelo (EMA)\n",
    "        ckpt = torch.load(best_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        best_thr = ckpt['thr']\n",
    "\n",
    "        # Métricas finales en VAL (con TTA)\n",
    "        probs_val, y_val = predict_loader(model, ld_va, tta=True)\n",
    "        vacc,vprec,vrec,vf1,vspec,vcm = metrics_from_preds(y_val, probs_val, best_thr)\n",
    "        cms.append(vcm)\n",
    "        plot_cm(vcm, f'Fold {fold} - Confusion Matrix (Val)', os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_fold{fold}_cm.png\"))\n",
    "\n",
    "        # Métricas en TRAIN (para registro)\n",
    "        probs_tr, y_tr = predict_loader(model, ld_tr, tta=False)\n",
    "        tacc,tprec,trec,tf1,tspec,tcm = metrics_from_preds(y_tr, probs_tr, best_thr)\n",
    "\n",
    "        fold_metrics_val.append({'Fold': fold, 'Accuracy': vacc, 'Precision': vprec, 'Recall': vrec, 'F1-Score': vf1, 'Specificity': vspec, 'BestThr': best_thr})\n",
    "        fold_metrics_train.append({'Fold': fold, 'Accuracy': tacc, 'Precision': tprec, 'Recall': trec, 'F1-Score': tf1, 'Specificity': tspec, 'BestThr': best_thr})\n",
    "\n",
    "        # limpieza\n",
    "        del model, ema, optimizer, ld_tr, ld_va, ds_tr, ds_va; torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    # Guardar historia por época\n",
    "    pd.DataFrame(history_rows).to_csv(os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_history_per_epoch.csv\"), index=False)\n",
    "    return fold_metrics_train, fold_metrics_val, cms\n",
    "\n",
    "print(\"✓ Funciones de entrenamiento definidas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def save_fold_metrics(train_list, val_list):\n",
    "    df_tr = pd.DataFrame(train_list)\n",
    "    df_va = pd.DataFrame(val_list)\n",
    "\n",
    "    def append_mean_std(df):\n",
    "        row = {'Fold':'Mean ± Std'}\n",
    "        for col in ['Accuracy','Precision','Recall','F1-Score','Specificity','BestThr']:\n",
    "            vals = df[col].astype(float).values\n",
    "            row[col] = f\"{vals.mean():.4f} ± {vals.std():.4f}\" if col!='BestThr' else f\"{vals.mean():.3f} ± {vals.std():.3f}\"\n",
    "        return pd.concat([df, pd.DataFrame([row])], axis=0)\n",
    "\n",
    "    df_tr_out = append_mean_std(df_tr)\n",
    "    df_va_out = append_mean_std(df_va)\n",
    "\n",
    "    path_tr = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_metrics_train.csv\")\n",
    "    path_va = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_metrics_val.csv\")\n",
    "    df_tr_out.to_csv(path_tr, index=False); df_va_out.to_csv(path_va, index=False)\n",
    "\n",
    "    print(\"✅ Guardadas métricas:\")\n",
    "    print(\"  Train  ->\", path_tr)\n",
    "    print(\"  Val    ->\", path_va)\n",
    "    return df_tr_out, df_va_out\n",
    "\n",
    "def save_mean_confusion_matrix(cms):\n",
    "    avg_cm = np.mean(np.stack(cms, axis=0), axis=0)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=['Benign','Malignant'], yticklabels=['Benign','Malignant'])\n",
    "    plt.title('Mean Confusion Matrix (Validation, 5-Fold)')\n",
    "    plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    plt.tight_layout()\n",
    "    # Nombre estándar y el nombre que me pediste explícitamente\n",
    "    p1 = os.path.join(CONFIG['metrics_dir'], f\"{CONFIG['model_name']}_avg_cm.png\")\n",
    "    p2 = os.path.join(CONFIG['metrics_dir'], \"CvT_TL_1_mean_confusion_matrix.png\")\n",
    "    plt.savefig(p1, dpi=150); plt.savefig(p2, dpi=150); plt.close()\n",
    "    print(\"✅ Mean CM guardado en:\")\n",
    "    print(\" \", p1)\n",
    "    print(\" \", p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print(\"=\"*80)\n",
    "print(\">>> ENTRENANDO CvT_TL_1 (512x512) con 5-Fold CV (Stratified por paciente)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start = time.time()\n",
    "train_list, val_list, cms = train_cv()\n",
    "df_tr, df_va = save_fold_metrics(train_list, val_list)\n",
    "save_mean_confusion_matrix(cms)\n",
    "print(f\"\\n⏱️ Tiempo total: {(time.time()-start)/3600:.2f} h\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
