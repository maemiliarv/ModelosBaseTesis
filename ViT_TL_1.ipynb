{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1) Imports y setup\n",
    "import os, json, math, random, warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from transformers import ViTModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = False\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2) Paths y CONFIG\n",
    "BASE_DIR   = Path(\"/home/merivadeneira\")\n",
    "MASAS_DIR  = BASE_DIR / \"Masas\"\n",
    "OUTPUT_DIR = BASE_DIR / \"Outputs\" / \"ViT\"\n",
    "METRICS_DIR= BASE_DIR / \"Metrics\" / \"ViT\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': 'ViT_TL_1',\n",
    "    'pretrained_model': 'google/vit-base-patch16-224',\n",
    "    'input_size': 224,\n",
    "    'num_classes': 2,\n",
    "    # entrenamiento\n",
    "    'k_folds': 5,\n",
    "    'batch_size': 32,\n",
    "    'max_epochs': 60,\n",
    "    'early_stopping_patience': 15,      # monitor F1\n",
    "    'freeze_epochs': 5,                 # solo cabeza\n",
    "    'unfreeze_after_epoch': 5,          # alias\n",
    "    # optim\n",
    "    'base_lr': 1e-4,                    # cabeza\n",
    "    'backbone_lr': 1e-5,                # al descongelar\n",
    "    'weight_decay': 0.05,\n",
    "    'grad_clip_norm': 1.0,\n",
    "    # augments\n",
    "    'rotation_degrees': 7,\n",
    "    'use_flip': False,                  # True si lateridad no importa\n",
    "    # eval\n",
    "    'use_tta': False,                   # se puede activar si quieres\n",
    "    # normalización (ImageNet)\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std' : [0.229, 0.224, 0.225],\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 3) Utilidades: carga de rutas y splits por paciente + dominio\n",
    "def extract_patient_id(filename, database):\n",
    "    if database == 'DDSM':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"DDSM_{parts[1]}\"\n",
    "    elif database == 'INbreast':\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 1:\n",
    "            return f\"INbreast_{parts[0]}\"\n",
    "    return filename\n",
    "\n",
    "def load_image_paths_with_patient_ids():\n",
    "    rows = []\n",
    "    for db in ['DDSM','INbreast']:\n",
    "        for label_name, label_val in [('Benignas',0), ('Malignas',1)]:\n",
    "            p = MASAS_DIR / db / label_name / \"Resized_512\"\n",
    "            if not p.exists(): continue\n",
    "            for img_file in p.glob(\"*.png\"):\n",
    "                pid = extract_patient_id(img_file.name, db)\n",
    "                rows.append({'image_path': str(img_file),\n",
    "                             'patient_id': pid,\n",
    "                             'label': label_val,\n",
    "                             'database': db})\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"Total imágenes: {len(df)} | Pacientes únicos: {df.patient_id.nunique()}\")\n",
    "    print(df.groupby(['database','label']).size().unstack(fill_value=0))\n",
    "    return df\n",
    "\n",
    "def create_patient_level_splits_stratified(data_df, k_folds=5, random_state=42):\n",
    "    # label por paciente + dominio mayoritario por paciente\n",
    "    grp = data_df.groupby('patient_id').agg(\n",
    "        label=('label', lambda x: int(x.mode()[0] if len(x.mode()) else x.iloc[0])),\n",
    "        database=('database', lambda x: x.mode()[0])\n",
    "    ).reset_index()\n",
    "    grp['strata'] = grp['label'].astype(str) + \"_\" + grp['database']\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    splits = []\n",
    "    for tr_idx, va_idx in skf.split(grp['patient_id'], grp['strata']):\n",
    "        tr_pat = set(grp.iloc[tr_idx]['patient_id'])\n",
    "        va_pat = set(grp.iloc[va_idx]['patient_id'])\n",
    "        train_idx = data_df.index[data_df['patient_id'].isin(tr_pat)].tolist()\n",
    "        val_idx   = data_df.index[data_df['patient_id'].isin(va_pat)].tolist()\n",
    "        splits.append((train_idx, val_idx))\n",
    "        print(f\"Fold -> train imgs: {len(train_idx)} | val imgs: {len(val_idx)}\")\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4) Dataset y transforms\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, df, indices, transform):\n",
    "        self.df = df.iloc[indices].reset_index(drop=True)\n",
    "        self.t  = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        p = self.df.loc[i, 'image_path']\n",
    "        y = int(self.df.loc[i, 'label'])\n",
    "        img = Image.open(p).convert('L')\n",
    "        # Grayscale -> 3 canales\n",
    "        img = img.convert('RGB')\n",
    "        x = self.t(img)\n",
    "        return x, y\n",
    "\n",
    "def make_transforms(cfg, train=True):\n",
    "    if train:\n",
    "        aug = [\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=cfg['input_size'],\n",
    "                scale=(0.90, 1.0),\n",
    "                ratio=(0.95, 1.05)\n",
    "            ),\n",
    "            transforms.RandomRotation(cfg['rotation_degrees']),\n",
    "        ]\n",
    "        if cfg['use_flip']:\n",
    "            aug.append(transforms.RandomHorizontalFlip(p=0.5))\n",
    "        aug += [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cfg['mean'], std=cfg['std'])\n",
    "        ]\n",
    "        return transforms.Compose(aug)\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((cfg['input_size'], cfg['input_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cfg['mean'], std=cfg['std'])\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 5) Modelo: ViT + pooling híbrido (CLS + mean)\n",
    "class ViTForMammography(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes=2, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(pretrained_model_name)\n",
    "        if freeze_backbone:\n",
    "            for p in self.vit.parameters():\n",
    "                p.requires_grad = False\n",
    "        h = self.vit.config.hidden_size  # 768 en vit-base\n",
    "        self.proj = nn.Linear(h*2, h)    # fusion CLS+MEAN\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(h, int(h*0.75)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(int(h*0.75), num_classes)\n",
    "        )\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for p in self.vit.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vit(pixel_values=x)\n",
    "        cls = out.last_hidden_state[:, 0]              # [B, h]\n",
    "        mean = out.last_hidden_state[:, 1:].mean(1)    # [B, h]\n",
    "        feat = torch.cat([cls, mean], dim=1)\n",
    "        feat = self.proj(feat)\n",
    "        logits = self.classifier(feat)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 6) Métricas, pérdida con pesos, TTA opcional, validación y utilidades\n",
    "def compute_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc  = (y_pred == y_true).mean()\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    cm   = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    return dict(accuracy=acc, f1=f1, precision=prec, recall=rec,\n",
    "                specificity=spec, cm=cm, tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_logits(model, loader, use_tta=False):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.numpy()\n",
    "        if not use_tta:\n",
    "            with autocast(enabled=(device.type=='cuda')):\n",
    "                logits = model(x)\n",
    "        else:\n",
    "            # TTA: original, hflip, vflip (promedio de logits)\n",
    "            logits_list = []\n",
    "            for variant in ['orig','h','v']:\n",
    "                x_i = x\n",
    "                if variant == 'h': x_i = torch.flip(x, dims=[-1])\n",
    "                if variant == 'v': x_i = torch.flip(x, dims=[-2])\n",
    "                with autocast(enabled=(device.type=='cuda')):\n",
    "                    logits_list.append(model(x_i))\n",
    "            logits = torch.stack(logits_list, dim=0).mean(0)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(y)\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    probs = all_logits.softmax(1)[:,1].numpy()\n",
    "    return probs, all_labels\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    # barrido fino de 0.05 a 0.95\n",
    "    thrs = np.linspace(0.05, 0.95, 37)\n",
    "    f1s = [f1_score(y_true, (y_prob>=t).astype(int), zero_division=0) for t in thrs]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(thrs[i]), float(f1s[i])\n",
    "\n",
    "def make_class_weights(df_indices, data_df):\n",
    "    y = data_df.iloc[df_indices]['label'].values\n",
    "    cls_vals, counts = np.unique(y, return_counts=True)\n",
    "    weights = {c: 1.0/max(cnt,1) for c, cnt in zip(cls_vals, counts)}\n",
    "    w = np.array([weights[val] for val in y], dtype=np.float32)\n",
    "    # normalizar\n",
    "    w = w / w.sum() * len(w)\n",
    "    return torch.as_tensor(w, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 7) Entrenamiento por fold (AMP + clipping + early stop por F1)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.best = -np.inf if mode=='max' else np.inf\n",
    "        self.count = 0\n",
    "        self.stop = False\n",
    "    def __call__(self, value):\n",
    "        improved = (value > self.best) if self.mode=='max' else (value < self.best)\n",
    "        if improved:\n",
    "            self.best = value; self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience: self.stop = True\n",
    "        return self.stop\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, grad_clip):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=(device.type=='cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        if grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()*x.size(0)\n",
    "        total += y.size(0)\n",
    "        total_correct += (logits.argmax(1)==y).sum().item()\n",
    "    return total_loss/total, total_correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 8) Bucle de entrenamiento de un fold (incluye mejor modelo por F1)\n",
    "def train_fold(fold_idx, train_idx, val_idx, data_df, cfg):\n",
    "    print(f\"\\n========== FOLD {fold_idx+1} ==========\")\n",
    "\n",
    "    # datasets y dataloaders\n",
    "    t_train = make_transforms(cfg, train=True)\n",
    "    t_val   = make_transforms(cfg, train=False)\n",
    "    ds_tr = MammographyDataset(data_df, train_idx, t_train)\n",
    "    ds_va = MammographyDataset(data_df, val_idx, t_val)\n",
    "\n",
    "    # sampler ponderado por clase\n",
    "    weights = make_class_weights(train_idx, data_df)\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg['batch_size'], sampler=sampler,\n",
    "                       num_workers=4, pin_memory=True, drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=cfg['batch_size'], shuffle=False,\n",
    "                       num_workers=4, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # modelo\n",
    "    model = ViTForMammography(pretrained_model_name=cfg['pretrained_model'],\n",
    "                              num_classes=cfg['num_classes'] if 'num_classes' in cfg else 2,\n",
    "                              freeze_backbone=True).to(device)\n",
    "\n",
    "    # optim: primero solo cabeza\n",
    "    head_params = list(model.proj.parameters()) + list(model.classifier.parameters())\n",
    "    optimizer = AdamW(head_params, lr=cfg['base_lr'], weight_decay=cfg['weight_decay'])\n",
    "    scaler = GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "    # pérdida con pesos (globales del fold)\n",
    "    y_tr = data_df.iloc[train_idx]['label'].values\n",
    "    cls_counts = np.bincount(y_tr, minlength=2).astype(float)\n",
    "    inv = 1.0 / np.maximum(cls_counts, 1.0)\n",
    "    class_weights_ce = torch.tensor(inv / inv.sum() * 2.0, dtype=torch.float32, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_ce)\n",
    "\n",
    "    history = {'train_loss':[], 'train_acc':[], 'val_f1':[], 'val_acc':[]}\n",
    "    es = EarlyStopping(patience=cfg['early_stopping_patience'], mode='max')\n",
    "    best_state, best_f1 = None, -1.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(cfg['max_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg['max_epochs']}\")\n",
    "        # unfreeze a partir de freeze_epochs\n",
    "        if epoch == cfg['unfreeze_after_epoch']:\n",
    "            print(\">> Descongelando backbone para fine-tuning\")\n",
    "            model.unfreeze_backbone()\n",
    "            # nuevo optimizer: backbone LR menor\n",
    "            optimizer = AdamW([\n",
    "                {'params': model.vit.parameters(), 'lr': cfg['backbone_lr']},\n",
    "                {'params': model.proj.parameters(), 'lr': cfg['base_lr']},\n",
    "                {'params': model.classifier.parameters(), 'lr': cfg['base_lr']},\n",
    "            ], weight_decay=cfg['weight_decay'])\n",
    "\n",
    "        tr_loss, tr_acc = train_one_epoch(model, dl_tr, criterion, optimizer, scaler, cfg['grad_clip_norm'])\n",
    "        history['train_loss'].append(tr_loss); history['train_acc'].append(tr_acc)\n",
    "\n",
    "        # validación (umbral 0.5 para early stop)\n",
    "        probs, y_true = infer_logits(model, dl_va, use_tta=cfg['use_tta'])\n",
    "        mets = compute_metrics(y_true, probs, thr=0.5)\n",
    "        history['val_f1'].append(mets['f1']); history['val_acc'].append(mets['accuracy'])\n",
    "\n",
    "        print(f\"Train: loss {tr_loss:.4f} | acc {tr_acc:.4f}\")\n",
    "        print(f\"Val  : F1 {mets['f1']:.4f} | acc {mets['accuracy']:.4f} | prec {mets['precision']:.4f} | rec {mets['recall']:.4f} | spec {mets['specificity']:.4f}\")\n",
    "\n",
    "        if mets['f1'] > best_f1:\n",
    "            best_f1 = mets['f1']; best_epoch = epoch+1\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            print(\"✓ Nuevo mejor modelo (por F1)\")\n",
    "\n",
    "        if es(mets['f1']):\n",
    "            print(f\"✓ Early stopping (paciencia agotada). Último mejor F1 en epoch {best_epoch}\")\n",
    "            break\n",
    "\n",
    "    # cargar mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # métricas finales con umbral óptimo (y también con 0.5 si quieres guardar)\n",
    "    probs_val, y_val = infer_logits(model, dl_va, use_tta=cfg['use_tta'])\n",
    "    thr_opt, f1_opt = find_best_threshold(y_val, probs_val)\n",
    "    mets_val = compute_metrics(y_val, probs_val, thr=thr_opt); mets_val['threshold']=thr_opt\n",
    "\n",
    "    # métricas en train (mismo umbral óptimo del val por consistencia)\n",
    "    probs_tr, y_tr_full = infer_logits(model, dl_tr, use_tta=cfg['use_tta'])\n",
    "    mets_tr = compute_metrics(y_tr_full, probs_tr, thr=thr_opt); mets_tr['threshold']=thr_opt\n",
    "\n",
    "    # guardar modelo del fold\n",
    "    model_path = OUTPUT_DIR / f\"{CONFIG['model_name']}_fold{fold_idx}.pth\"\n",
    "    torch.save({'fold': fold_idx, 'state_dict': model.state_dict(), 'config': cfg,\n",
    "                'threshold': thr_opt, 'best_epoch': best_epoch}, model_path)\n",
    "    print(\"Modelo guardado:\", model_path)\n",
    "\n",
    "    return model, history, mets_tr, mets_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 9) Orquestador K-Fold + escritura del CSV ViT_TL_1_metrics\n",
    "def main():\n",
    "    print(\"\\n=== INICIO ENTRENAMIENTO: ViT_TL_1 ===\")\n",
    "    data_df = load_image_paths_with_patient_ids()\n",
    "    splits = create_patient_level_splits_stratified(data_df, k_folds=CONFIG['k_folds'])\n",
    "\n",
    "    # DataFrame de métricas por fold/split\n",
    "    rows = []\n",
    "    all_val_cms = []\n",
    "\n",
    "    for f, (tr_idx, va_idx) in enumerate(splits):\n",
    "        torch.cuda.empty_cache()\n",
    "        model, hist, mets_tr, mets_va = train_fold(f, tr_idx, va_idx, data_df, CONFIG)\n",
    "\n",
    "        def row_from_metrics(split, m, epochs):\n",
    "            return {\n",
    "                'fold': f+1, 'split': split,\n",
    "                'loss': np.nan,                  # opcional si quieres registrar otra cosa\n",
    "                'accuracy': m['accuracy'],\n",
    "                'precision': m['precision'],\n",
    "                'recall': m['recall'],\n",
    "                'specificity': m['specificity'],\n",
    "                'f1': m['f1'],\n",
    "                'threshold_used': m.get('threshold', 0.5),\n",
    "                'tn': m['tn'], 'fp': m['fp'], 'fn': m['fn'], 'tp': m['tp'],\n",
    "                'samples': (m['tn']+m['fp']+m['fn']+m['tp']),\n",
    "                'epochs_trained': epochs,\n",
    "                'backbone_unfreeze_epoch': CONFIG['unfreeze_after_epoch'],\n",
    "                'tta': CONFIG['use_tta']\n",
    "            }\n",
    "\n",
    "        rows.append(row_from_metrics('train', mets_tr, epochs=len(hist['train_loss'])))\n",
    "        rows.append(row_from_metrics('val',   mets_va, epochs=len(hist['train_loss'])))\n",
    "        all_val_cms.append(mets_va['cm'])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"CSV de métricas por fold guardado en:\", csv_path)\n",
    "\n",
    "    # promedios por split\n",
    "    summary = df.groupby('split')[['accuracy','precision','recall','specificity','f1']].agg(['mean','std'])\n",
    "    summary_path = METRICS_DIR / f\"{CONFIG['model_name']}_metrics_summary.csv\"\n",
    "    summary.to_csv(summary_path)\n",
    "    print(\"Resumen (mean ± std) guardado en:\", summary_path)\n",
    "\n",
    "    # añadir filas MEAN al CSV principal (al final)\n",
    "    mean_rows = []\n",
    "    for split in ['train','val']:\n",
    "        sub = df[df['split']==split]\n",
    "        mean_rows.append({\n",
    "            'fold':'MEAN','split':split,\n",
    "            'loss':np.nan,\n",
    "            'accuracy':sub['accuracy'].mean(),\n",
    "            'precision':sub['precision'].mean(),\n",
    "            'recall':sub['recall'].mean(),\n",
    "            'specificity':sub['specificity'].mean(),\n",
    "            'f1':sub['f1'].mean(),\n",
    "            'threshold_used':sub['threshold_used'].mean(),\n",
    "            'tn':sub['tn'].mean(),'fp':sub['fp'].mean(),\n",
    "            'fn':sub['fn'].mean(),'tp':sub['tp'].mean(),\n",
    "            'samples':sub['samples'].mean(),\n",
    "            'epochs_trained':sub['epochs_trained'].mean(),\n",
    "            'backbone_unfreeze_epoch':CONFIG['unfreeze_after_epoch'],\n",
    "            'tta':CONFIG['use_tta']\n",
    "        })\n",
    "    df_mean_appended = pd.concat([df, pd.DataFrame(mean_rows)], ignore_index=True)\n",
    "    df_mean_appended.to_csv(csv_path, index=False)\n",
    "    print(\"CSV actualizado con filas MEAN.\")\n",
    "\n",
    "    # matriz de confusión promedio (validación)\n",
    "    mean_cm = np.mean(np.stack(all_val_cms, axis=0), axis=0)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(mean_cm, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=['Benigna','Maligna'], yticklabels=['Benigna','Maligna'])\n",
    "    plt.title(f'Average Confusion Matrix (VAL) — {CONFIG[\"model_name\"]}')\n",
    "    out_cm = METRICS_DIR / f\"{CONFIG['model_name']}_mean_confusion_matrix.png\"\n",
    "    plt.tight_layout(); plt.savefig(out_cm, dpi=300); plt.close()\n",
    "    print(\"Matriz de confusión promedio guardada en:\", out_cm)\n",
    "\n",
    "    # guardar config\n",
    "    with open(OUTPUT_DIR / f\"{CONFIG['model_name']}_config.json\",'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== ENTRENAMIENTO COMPLETADO ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
